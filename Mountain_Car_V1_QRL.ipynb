{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSoeBdTwpUF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgTTkiY0wpUG"
      },
      "source": [
        "Install TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bPTH8ScrwpUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125d1420-59f2-4952-deb7-de08c3c349ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlbSE9jXwpUH"
      },
      "source": [
        "Install TensorFlow Quantum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MZeJimx6wpUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05cb98c1-988e-4dc3-b370-0b03e111d96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-quantum==0.7.3 in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: cirq-core==1.3.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-quantum==0.7.3) (1.3.0)\n",
            "Requirement already satisfied: cirq-google==1.3.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-quantum==0.7.3) (1.3.0)\n",
            "Requirement already satisfied: sympy==1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow-quantum==0.7.3) (1.12)\n",
            "Requirement already satisfied: duet~=0.2.8 in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (0.2.9)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (3.3)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2.0.3)\n",
            "Requirement already satisfied: sortedcontainers~=2.0 in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (4.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cirq-core==1.3.0->tensorflow-quantum==0.7.3) (4.66.4)\n",
            "Requirement already satisfied: google-api-core[grpc]>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from cirq-google==1.3.0->tensorflow-quantum==0.7.3) (2.11.1)\n",
            "Requirement already satisfied: proto-plus>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.23.0)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from cirq-google==1.3.0->tensorflow-quantum==0.7.3) (3.20.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy==1.12->tensorflow-quantum==0.7.3) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (2.27.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (1.48.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (2024.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->cirq-core==1.3.0->tensorflow-quantum==0.7.3) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (2024.2.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]>=1.14.0->cirq-google==1.3.0->tensorflow-quantum==0.7.3) (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-quantum==0.7.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLEu0I6qwpUI"
      },
      "source": [
        "Install Gym:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6A2JRKhMwpUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e197389-742b-4837-d809-27cdfb00b2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.18.0\n",
            "  Using cached gym-0.18.0.tar.gz (1.6 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id8vB7FiwpUJ"
      },
      "source": [
        "Now import TensorFlow and the module dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4Ql5PW-ACO0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661b82b4-c1fd-44fe-ff0a-473053abd8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:121: DeprecationWarning: pkg_resources is deprecated as an API\n",
            "  warnings.warn(\"pkg_resources is deprecated as an API\", DeprecationWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:2870: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pkg_resources' from '/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RIIYRJ79wpUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13d31b2-7940-4e4a-f30c-bf7e1ff5fcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "\n",
        "import gym, cirq, sympy\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from collections import deque, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from cirq.contrib.svg import SVGCircuit\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfbzHvkuB6tt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxWGru_NwpUK"
      },
      "source": [
        "## 1. Build a PQC with data re-uploading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85woLQQswpUL"
      },
      "source": [
        "At the core of both RL algorithms you are implementing is a PQC that takes as input the agent's state $s$ in the environment (i.e., a numpy array) and outputs a vector of expectation values. These expectation values are then post-processed, either to produce an agent's policy $\\pi(a|s)$ or approximate Q-values $Q(s,a)$. In this way, the PQCs are playing an analog role to that of deep neural networks in modern deep RL algorithms.\n",
        "\n",
        "A popular way to encode an input vector in a PQC is through the use of single-qubit rotations, where rotation angles are controlled by the components of this input vector. In order to get a [highly-expressive model](https://arxiv.org/abs/2008.08605), these single-qubit encodings are not performed only once in the PQC, but in several \"[re-uploadings](https://quantum-journal.org/papers/q-2020-02-06-226/)\", interlayed with variational gates. The layout of such a PQC is depicted below:\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/quantum/blob/master/docs/tutorials/images/pqc_re-uploading.png?raw=1\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxw3Rz0awpUL"
      },
      "source": [
        "As discussed in [[1]](https://arxiv.org/abs/2103.05577) and [[2]](https://arxiv.org/abs/2103.15084), a way to further enhance the expressivity and trainability of data re-uploading PQCs is to use trainable input-scaling parameters $\\boldsymbol{\\lambda}$ for each encoding gate of the PQC, and trainable observable weights $\\boldsymbol{w}$ at its output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNSjI-OywpUM"
      },
      "source": [
        "### 1.1 Cirq circuit for ControlledPQC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCYrUUwswpUM"
      },
      "source": [
        "The first step is to implement in Cirq the quantum circuit to be used as the PQC. For this, start by defining basic unitaries to be applied in the circuits, namely an arbitrary single-qubit rotation and an entangling layer of CZ gates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X4P5EORYwpUM"
      },
      "outputs": [],
      "source": [
        "def one_qubit_rotation(qubit, symbols):\n",
        "    \"\"\"\n",
        "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
        "    Y and Z axis, specified by the values in `symbols`.\n",
        "    \"\"\"\n",
        "    return [cirq.rx(symbols[0])(qubit),\n",
        "            cirq.ry(symbols[1])(qubit),\n",
        "            cirq.rz(symbols[2])(qubit)]\n",
        "\n",
        "def entangling_layer(qubits):\n",
        "    \"\"\"\n",
        "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
        "    \"\"\"\n",
        "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
        "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
        "    return cz_ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTgpkm6iwpUM"
      },
      "source": [
        "Now, use these functions to generate the Cirq circuit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PEicpzq9wpUN"
      },
      "outputs": [],
      "source": [
        "def generate_circuit(qubits, n_layers):\n",
        "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
        "    # Number of qubits\n",
        "    n_qubits = len(qubits)\n",
        "\n",
        "    # Sympy symbols for variational angles\n",
        "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
        "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
        "\n",
        "    # Sympy symbols for encoding angles\n",
        "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
        "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
        "\n",
        "    # Define circuit\n",
        "    circuit = cirq.Circuit()\n",
        "    for l in range(n_layers):\n",
        "        # Variational layer\n",
        "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
        "        circuit += entangling_layer(qubits)\n",
        "        # Encoding layer\n",
        "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
        "\n",
        "    # Last varitional layer\n",
        "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
        "\n",
        "    return circuit, list(params.flat), list(inputs.flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL8MvT21wpUN"
      },
      "source": [
        "Check that this produces a circuit that is alternating between variational and encoding layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "M4LFL2bQwpUO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11a84bfa-c22f-48cc-9f3e-f290532b08f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n",
            "WARNING:matplotlib.font_manager:findfont: Font family 'Arial' not found.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cirq.contrib.svg.svg.SVGCircuit at 0x7cbfd10a6170>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1222.8741406250003\" height=\"300.0\"><line x1=\"34.7588671875\" x2=\"1192.8741406250003\" y1=\"25.0\" y2=\"25.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"1192.8741406250003\" y1=\"75.0\" y2=\"75.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"1192.8741406250003\" y1=\"125.0\" y2=\"125.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"1192.8741406250003\" y1=\"175.0\" y2=\"175.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"1192.8741406250003\" y1=\"225.0\" y2=\"225.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"34.7588671875\" x2=\"1192.8741406250003\" y1=\"275.0\" y2=\"275.0\" stroke=\"#1967d2\" stroke-width=\"1\" /><line x1=\"431.46617187500004\" x2=\"431.46617187500004\" y1=\"25.0\" y2=\"75.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"491.46617187500004\" x2=\"491.46617187500004\" y1=\"75.0\" y2=\"125.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"551.4661718750001\" x2=\"551.4661718750001\" y1=\"125.0\" y2=\"175.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"611.4661718750001\" x2=\"611.4661718750001\" y1=\"175.0\" y2=\"225.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"671.4661718750001\" x2=\"671.4661718750001\" y1=\"225.0\" y2=\"275.0\" stroke=\"black\" stroke-width=\"3\" /><line x1=\"731.4661718750001\" x2=\"731.4661718750001\" y1=\"25.0\" y2=\"275.0\" stroke=\"black\" stroke-width=\"3\" /><rect x=\"10.0\" y=\"5.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">(0, 0): </text><rect x=\"10.0\" y=\"55.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">(0, 1): </text><rect x=\"10.0\" y=\"105.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">(0, 2): </text><rect x=\"10.0\" y=\"155.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">(0, 3): </text><rect x=\"10.0\" y=\"205.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">(0, 4): </text><rect x=\"10.0\" y=\"255.0\" width=\"49.517734375\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"0\" /><text x=\"34.7588671875\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">(0, 5): </text><rect x=\"79.517734375\" y=\"5.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"125.12689453125\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta0)</text><rect x=\"79.517734375\" y=\"55.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"125.12689453125\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta3)</text><rect x=\"79.517734375\" y=\"105.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"125.12689453125\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta6)</text><rect x=\"79.517734375\" y=\"155.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"125.12689453125\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta9)</text><rect x=\"79.517734375\" y=\"205.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"125.12689453125\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta12)</text><rect x=\"79.517734375\" y=\"255.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"125.12689453125\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta15)</text><rect x=\"190.73605468750003\" y=\"5.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"235.96021484375004\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta1)</text><rect x=\"190.73605468750003\" y=\"55.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"235.96021484375004\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta4)</text><rect x=\"190.73605468750003\" y=\"105.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"235.96021484375004\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta7)</text><rect x=\"190.73605468750003\" y=\"155.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"235.96021484375004\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta10)</text><rect x=\"190.73605468750003\" y=\"205.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"235.96021484375004\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta13)</text><rect x=\"190.73605468750003\" y=\"255.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"235.96021484375004\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta16)</text><rect x=\"301.18437500000005\" y=\"5.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"346.3252734375001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta2)</text><rect x=\"301.18437500000005\" y=\"55.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"346.3252734375001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta5)</text><rect x=\"301.18437500000005\" y=\"105.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"346.3252734375001\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta8)</text><rect x=\"301.18437500000005\" y=\"155.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"346.3252734375001\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta11)</text><rect x=\"301.18437500000005\" y=\"205.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"346.3252734375001\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta14)</text><rect x=\"301.18437500000005\" y=\"255.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"346.3252734375001\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta17)</text><circle cx=\"431.46617187500004\" cy=\"25.0\" r=\"10.0\" /><circle cx=\"431.46617187500004\" cy=\"75.0\" r=\"10.0\" /><circle cx=\"491.46617187500004\" cy=\"75.0\" r=\"10.0\" /><circle cx=\"491.46617187500004\" cy=\"125.0\" r=\"10.0\" /><circle cx=\"551.4661718750001\" cy=\"125.0\" r=\"10.0\" /><circle cx=\"551.4661718750001\" cy=\"175.0\" r=\"10.0\" /><circle cx=\"611.4661718750001\" cy=\"175.0\" r=\"10.0\" /><circle cx=\"611.4661718750001\" cy=\"225.0\" r=\"10.0\" /><circle cx=\"671.4661718750001\" cy=\"225.0\" r=\"10.0\" /><circle cx=\"671.4661718750001\" cy=\"275.0\" r=\"10.0\" /><circle cx=\"731.4661718750001\" cy=\"25.0\" r=\"10.0\" /><circle cx=\"731.4661718750001\" cy=\"275.0\" r=\"10.0\" /><rect x=\"771.4661718750001\" y=\"5.0\" width=\"69.45953125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"806.1959375000001\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(x0_0)</text><rect x=\"771.4661718750001\" y=\"55.0\" width=\"69.45953125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"806.1959375000001\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(x0_1)</text><rect x=\"771.4661718750001\" y=\"105.0\" width=\"69.45953125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"806.1959375000001\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(x0_2)</text><rect x=\"771.4661718750001\" y=\"155.0\" width=\"69.45953125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"806.1959375000001\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(x0_3)</text><rect x=\"771.4661718750001\" y=\"205.0\" width=\"69.45953125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"806.1959375000001\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(x0_4)</text><rect x=\"771.4661718750001\" y=\"255.0\" width=\"69.45953125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"806.1959375000001\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(x0_5)</text><rect x=\"860.9257031250002\" y=\"5.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"906.5348632812502\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta18)</text><rect x=\"860.9257031250002\" y=\"55.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"906.5348632812502\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta21)</text><rect x=\"860.9257031250002\" y=\"105.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"906.5348632812502\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta24)</text><rect x=\"860.9257031250002\" y=\"155.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"906.5348632812502\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta27)</text><rect x=\"860.9257031250002\" y=\"205.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"906.5348632812502\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta30)</text><rect x=\"860.9257031250002\" y=\"255.0\" width=\"91.21832031250001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"906.5348632812502\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rx(theta33)</text><rect x=\"972.1440234375002\" y=\"5.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1017.3681835937502\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta19)</text><rect x=\"972.1440234375002\" y=\"55.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1017.3681835937502\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta22)</text><rect x=\"972.1440234375002\" y=\"105.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1017.3681835937502\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta25)</text><rect x=\"972.1440234375002\" y=\"155.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1017.3681835937502\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta28)</text><rect x=\"972.1440234375002\" y=\"205.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1017.3681835937502\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta31)</text><rect x=\"972.1440234375002\" y=\"255.0\" width=\"90.4483203125\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1017.3681835937502\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Ry(theta34)</text><rect x=\"1082.5923437500003\" y=\"5.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1127.7332421875003\" y=\"25.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta20)</text><rect x=\"1082.5923437500003\" y=\"55.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1127.7332421875003\" y=\"75.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta23)</text><rect x=\"1082.5923437500003\" y=\"105.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1127.7332421875003\" y=\"125.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta26)</text><rect x=\"1082.5923437500003\" y=\"155.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1127.7332421875003\" y=\"175.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta29)</text><rect x=\"1082.5923437500003\" y=\"205.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1127.7332421875003\" y=\"225.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta32)</text><rect x=\"1082.5923437500003\" y=\"255.0\" width=\"90.28179687500001\" height=\"40\" stroke=\"black\" fill=\"white\" stroke-width=\"1\" /><text x=\"1127.7332421875003\" y=\"275.0\" dominant-baseline=\"middle\" text-anchor=\"middle\" font-size=\"14px\" font-family=\"Arial:style=normal:variant=normal:weight=normal:stretch=normal:size=10.0\">Rz(theta35)</text></svg>"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "n_qubits, n_layers = 6, 1\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "circuit, _, _ = generate_circuit(qubits, n_layers)\n",
        "SVGCircuit(circuit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RrFUkT3wpUP"
      },
      "source": [
        "### 1.2 ReUploadingPQC layer using ControlledPQC\n",
        "\n",
        "To construct the re-uploading PQC from the figure above, you can create a custom Keras layer. This layer will manage the trainable parameters (variational angles $\\boldsymbol{\\theta}$ and input-scaling parameters $\\boldsymbol{\\lambda}$) and resolve the input values (input state $s$) into the appropriate symbols in the circuit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7XJvWgQ4wpUP"
      },
      "outputs": [],
      "source": [
        "class ReUploadingPQC(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
        "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
        "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
        "        then applied to all lmbd[i][j]s_i.\n",
        "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
        "        by the ControlledPQC.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
        "        super(ReUploadingPQC, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_qubits = len(qubits)\n",
        "\n",
        "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
        "\n",
        "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
        "        self.theta = tf.Variable(\n",
        "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
        "            trainable=True, name=\"thetas\"\n",
        "        )\n",
        "\n",
        "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
        "        self.lmbd = tf.Variable(\n",
        "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
        "        )\n",
        "\n",
        "        # Define explicit symbol order.\n",
        "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
        "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
        "\n",
        "        self.activation = activation\n",
        "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
        "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs[0] = encoding data for the state.\n",
        "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
        "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
        "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
        "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
        "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
        "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "\n",
        "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
        "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
        "\n",
        "        return self.computation_layer([tiled_up_circuits, joined_vars])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u3QBKbvwpUP"
      },
      "source": [
        "## 2. Policy-gradient RL with PQC policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4deMRl86wpUP"
      },
      "source": [
        "In this section, you will implement the policy-gradient algorithm presented in <a href=\"https://arxiv.org/abs/2103.05577\" class=\"external\">[1]</a>. For this, you will start by constructing, out of the PQC that was just defined, the `softmax-VQC` policy (where VQC stands for variational quantum circuit):\n",
        "$$ \\pi_\\theta(a|s) = \\frac{e^{\\beta \\langle O_a \\rangle_{s,\\theta}}}{\\sum_{a'} e^{\\beta \\langle O_{a'} \\rangle_{s,\\theta}}} $$\n",
        "where $\\langle O_a \\rangle_{s,\\theta}$ are expectation values of observables $O_a$ (one per action) measured at the output of the PQC, and $\\beta$ is a tunable inverse-temperature parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb7zQF5AwpUQ"
      },
      "source": [
        "You can adopt the same observables used in <a href=\"https://arxiv.org/abs/2103.05577\" class=\"external\">[1]</a> for CartPole, namely a global $Z_0Z_1Z_2Z_3$ Pauli product acting on all qubits, weighted by an action-specific weight for each action. To implement the weighting of the Pauli product, you can use an extra `tf.keras.layers.Layer` that stores the action-specific weights and applies them multiplicatively on the expectation value $\\langle Z_0Z_1Z_2Z_3 \\rangle_{s,\\theta}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kPLHsGRewpUQ"
      },
      "outputs": [],
      "source": [
        "class Alternating(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        super(Alternating, self).__init__()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.constant([[(-1.)**i for i in range(output_dim)]]), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdyTMNPTwpUQ"
      },
      "source": [
        "Prepare the definition of your PQC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "l3yZCMhywpUQ"
      },
      "outputs": [],
      "source": [
        "n_qubits = 3 # Dimension of the state vectors in Mountaain Car\n",
        "n_layers = 5 # Number of layers in the PQC\n",
        "n_actions = 2 # Number of actions in Mountaain Car\n",
        "\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMGNUCmOwpUR"
      },
      "source": [
        "and its observables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qMAc2_--wpUR"
      },
      "outputs": [],
      "source": [
        "ops = [cirq.Z(q) for q in qubits]\n",
        "observables = [reduce((lambda x, y: x * y), ops)] # Z_0*Z_1*Z_2*Z_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px9D6vE8wpUR"
      },
      "source": [
        "With this, define a `tf.keras.Model` that applies, sequentially, the `ReUploadingPQC` layer previously defined, followed by a post-processing layer that computes the weighted observables using `Alternating`, which are then fed into a `tf.keras.layers.Softmax` layer that outputs the `softmax-VQC` policy of the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-ivAvce6wpUR"
      },
      "outputs": [],
      "source": [
        "def generate_model_policy(qubits, n_layers, n_actions, beta, observables):\n",
        "    \"\"\"Generates a Keras model for a data re-uploading PQC policy.\"\"\"\n",
        "\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables)([input_tensor])\n",
        "    process = tf.keras.Sequential([\n",
        "        Alternating(n_actions),\n",
        "        tf.keras.layers.Lambda(lambda x: x * beta),\n",
        "        tf.keras.layers.Softmax()\n",
        "    ], name=\"observables-policy\")\n",
        "    policy = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=policy)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = generate_model_policy(qubits, n_layers, n_actions, 1.0, observables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ANysIOrswpUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "ac4a94ab-f3f9-42c8-f40a-c8ee83eff452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAADXCAYAAAAtF3xmAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO2de1hTV7qHfxuEhCSAiWDwgkJlEHUsnlprnTpH62W0Y71MS0G0eGMUW6VV7KgHsVJlWjpT9SlUpFZFZbzWsRfqg4I4eKGix0d7Wm8IShURKCIIBGJC8p0/HDJEEkggN3bX+zz8wdp7r/WttffHXntl54UjIgKDweAFTvYOgMFgWA6W0AwGj+hmycoePXqEuro6S1bJYPAaiUSC7t27W6w+zpLP0DExMfjHP/4BDw8PS1XJWx49egQ3Nze4urraOxSLoVaroVAoLHqB8pm6ujqEhIRgy5YtFqvTondoAIiPj8fbb79t6Wp5x/Tp0/H2229j0qRJ9g7FYuTn52P16tXIzc21dyhdgh07duDSpUsWrZM9QzMYPIIlNIPBI1hCMxg8oksm9CuvvGLRhYSuAl/6LZFIwHEcOI7DjRs3ADxZUEtMTERxcTE2bdqk22fTpk26486ePYu+ffvC1dUV8+fPt1m8SqUSQ4YMgUgkgkQiwUsvvYTz58/j8OHDyMzM1O0XHx+v69e8efNsFl9LumRCZ2ZmYsmSJVape926dbh3755V6u4sfOp3RkYGfvnlFwQFBUGj0SA0NBTjx4+Hv78/YmJikJiYCD8/PyQkJODhw4cAgNGjRyM/Px9z5sxBWlqazWJtamrCCy+8gPLycty/fx+DBw9GeHg4QkJCcPbsWezcuRMAEBcXh+LiYrsuCnfJhLYmBw8etHcIdsHW/f7tb38Lb29vAMCHH36Inj17YsSIEXr7rF+/HkKhEBs2bLBpbE8jkUiQlpYGDw8PeHh4IDQ0FKWlpSAixMXFIT4+HteuXUO3bt3g5+cHX19fu8Xa5RJ6586dEAqFiIuLw6pVq8BxHN566y0MGjQIEokE69evB/DkM3GO4zBx4kRIJBL4+/vjyy+/BACEhoaC4zgUFRXh/v378PPzg0QiQVhYGAoKCuDr64ulS5fas5utaNlvAEb7bql+T506FcuXL7d6vzQaDVJTUzFnzpxW26RSKbZu3YqUlBTcunWr1fYTJ05g2LBhkEgkCA4OxvHjxwEYHxsAOHLkCAIDA+Hp6YkFCxZApVKZHKtWq0VFRQV2796NsLAwcBwHNzc3zJgxA9u2bevgCFgYsiDLly+nLVu2WLJKg0RGRtKaNWuIiEgul9OZM2dIq9XSrl27SCQS6fYTi8WUlZVFDQ0NlJqaSkKhkMrKyoiICAAVFhYSEdHly5dJLBaTWq0mAFRSUmL1PkybNo2OHTtm1jEt+01kvO/26ve5c+dozJgx7e4nFoupuLiYiIjy8/MJANXW1urtk5ycTBkZGUREFBYWRiEhIUREVFJSQpGRkVRZWUlisZj27t1LtbW1lJqaSmKxmCoqKoyOTVlZGbm5uVFGRgZVVVXR8OHDKSkpyeT+TZo0iQDQ+PHjde0QEe3cuZP8/f11v3/00Uc0d+7cduvbvn07vf322ya3bwpd7g5tDI7jMGbMGDQ0NKCpqUlX3qtXL7i5uSEqKgo9evTg5UsPhvreVfpdXFwMgUAAd3d3o/skJyfj1KlTOHfunK4sOzsbcrkcs2bNgru7u66fJ0+e1Du25dicOHECvr6+ePXVVyGTyTBt2jScPn3a5FiPHj2K8vJyTJgwAc899xyqq6sBAF5eXrhz5w7IAb64yJuENoWePXuiqqrK3mHYHEfud2NjIwQCQZv7eHt7IykpCStWrNCVVVRU6J7Bm5HL5aioqDBaT2VlJW7evKlbiV63bh1qampMjtXZ2RlyuRyrV6+Gq6srvv32WwCAQCCAVquFUqk0uS5r8atJaCJCaWkp+vTpY+9QbIqj91skEpn0HDtz5kzI5XLdeoCPjw8qKyv19ikvL4ePj4/ROqRSKYYOHQoi0v1kZ2d3KO6Wd2OVSgUnJycIhcIO1WVJeJ/Q9fX1UCqVSE5Ohkqlwrhx4wA8WbnMy8uDWq1GaWkpAMDJyQlOTk64fv06Ghoa7Bl2p+kq/fbz84NSqUR9fX27+6akpOg+h58wYQJ++eUX7N27F3V1dUhNTUV1dTUmTJhg9PixY8eioKAA+/btg0KhQENDg0l36G+++Qbbtm2DUqnEo0ePkJSUhMrKSrz88ssAntz5+/fvD47jTOy1FbHkA7ktFsVWrVpFrq6uJBKJCAABoP79+1NNTQ0NGTKEANDs2bOJ6Mnii0wmIxcXFwoODqbc3FxdPWvXriWhUEiBgYEUFRVFAGj+/PkUGhpKAoGAwsPDrdoPcxfFWvb7448/ppUrVxrtuyX6PWXKFHr33XfN6lNHFsWampqod+/elJ+fr9u+efNmkkgkJJPJaPv27XrHpqWlUWRkJBERZWVl0dChQ0kkElFwcDBlZ2cTEbU5NgcOHKDAwEASCAQ0cuRIunjxIlVWVpK3tzclJycbjPfixYvk7+9PAoGARCIRvfDCC5STk6Pbvnz5cr2xsueiWJdLaHMQi8V09epVe4dhkI6scpuKvfptTkJ/9913VFlZSURECQkJFBMTY+XojKNSqeiNN96ghIQEs49Vq9UUEBBA165do6amJvr555/p7bffZqvc1kKr1do7BLvg6P1+9dVX4e3tjRs3bmD16tW4ffs2zp8/b5dYUlNT4eXlhZiYGLOPjYuLQ2xsLAYNGoQNGzbAz88PKSkpVojSNHib0BEREVAoFJg8ebLFv3PqyHSFftfX1+sWpYKCguDs7IxDhw4hJycHxcXFNo8nOjoaKSkpcHNzM+u4gwcPYsyYMbr3yuPj43X92rVrlxUibR+LCw4chfT0dKSnp9s7DJvTVfvt4uKC2NhYe4dhFmFhYfYOoRW8vUMzGL9GWEIzGDyCJTSDwSMsav1cunQpsrKy0Lt3b0tVyVuKioogk8kgk8nsHYrFqK2tRWlpKQYNGmTvULoE5eXlGD16NLZv326xOi26KObi4oLp06fj9ddft2S1vGTVqlV47bXXMHLkSHuHYjGuXLmCrVu3IjEx0d6hdAkyMjJ08gZLYdGE5jgO/v7+ePHFFy1ZLS/p3r07goKCeDdW7u7uvOuTtbh69apZXw4xBfYMzWDwCJbQDAaPYAnNYPAImyf07t27IZVKwXEcAgICcPfuXau11VIHO2DAAOTn51utLYbpMI2v9bB5Qs+dO1enYC0qKkK/fv0s3kazkrZZByuXy3Hr1q1fzWJNZ5W8tlD6Mo2vdeDllPvXquJtprP9t8X4MY2vdbB7QndExWtMRwvAbBXvokWLIJVK4ebmhoiICGi1WkyZMgUcx8HPzw/379/HP//5T3h6emLw4MEADKtgo6OjwXEcMjMzERISYrEvGhhS1Zra/+bpnyMrfZnG18JY8svVpgoOvvrqK2rZdEdUvDCgoyWiVkra5ORkksvlRmOJjo6msrIyKiwsJBcXF7py5QopFAry9PSkr7/+WrdfVFQUlZWVtamClcvllJ6eTjU1NZSYmNjmGJgiOGhLVWtq/22p9GUa3ycwje+/sYeKNykpCT4+PggICIBMJkNdXR1EIhHCw8Oxb98+AE8WbNRqNXx8fJCbm9umCtbPzw+enp5YtWpVp2MzVVXbHo6s9GUaX8viUAltCpZU0tbV1WHmzJnw8vKCq6urngI2MjISGRkZqKurQ1ZWFl577TUAnVfBmkNHVLXt4WhKX6bxtSxdKqHJwkraPXv24Pr16/jhhx/Q2NgIuVyu2/b8888jMDAQX331FfLy8jB58mQAllXBtkdHVLVtYenxswRM42tZukRCG1LSGtLRAu0raYkINTU1iIqKwuPHjyEQCCCRSFBQUNDqL2xkZCTS0tIgk8ng7OwMoOMq2I7QlqrWnP47stKXaXwtjCUfyE1ZFNuzZw9JpVICQL/5zW8oPDy8QypeYzpaItIpaQGQRCLR6X5b/sybN4/u3LlDAwcOJLFYTOHh4TRgwAAaMGAAaTQaIiJ6+PAhiUQi3QJOM4ZUsNHR0QSAevXqRXl5ee2OlanWT2OqWlP6Hx4eblOlL9P4PoFpfNvAniperVZL77zzjlXqtqbGtyW2HD+m8WUaX5OwtZL2zJkzUCgUiI+Px/Tp023atjVwRKUv0/haB4dOaHspaVNSUuDj4wOO43TPm10RR1X6Mo2v9XBoja+9lLT79++3eZvWoCspfZnG1zI49B2awWCYB0toBoNHWHzK/f3337f75g8DuHv3Lo4fP271rynaktu3b6O8vBw7duywdyhdgrNnz0IkElm0TotqfI8cOYKcnBxLVcdrHj9+DBcXFzg5WX+SdOPGDWi1Wt23xayFVquFWq1mf9DNYMyYMQgNDbVYfRZNaIZjkpiYCKVSifj4eHuHwrAy7BmaweARLKEZDB7BEprB4BEsoRkMHsESmsHgESyhGQwewRKaweARLKEZDB7BEprB4BEsoRkMHsESmsHgESyhGQwewRKaweARLKEZDB7BEprB4BEsoRkMHsESmsHgESyhGQwewRKaweARLKEZDB7BEprB4BEsoRkMHsESmsHgEQ79z+oYHUOr1eLkyZO634uKiqBSqXDixAld2dixY9GtGzv9fIOJ9nlKUFAQSktL4erqiuZTzHEc1Go1evToYZd/28qwPmzKzVPmzJmDpqYmPHz4ENXV1aiursbDhw/x+PFjRERE2Ds8hpVgd2ieUlJSgqCgIDQ0NOiVSyQSXLp0Cb/5zW/sFBnDmrA7NE/x9fVFQEBAq/I+ffqwZOYxLKF5TFRUFMRise53Nzc3LFq0yI4RMawNm3LzmAcPHqBfv35obGwEAIhEIty8eRN9+vSxc2QMa8Hu0DzGy8sLzz33nO73wYMHs2TmOSyhec7ixYvh4eEBsViMqKgoe4fDsDJsys1z6urq4OPjAyJCaWkppFKpvUNiWBG9V4VOnz6NK1eu2CsWhpXw9/eHWq3G/v377R0Kw8IEBQVh3Lhxut/1Enrfvn34+eefMXjwYJsHxrA8u3fvxsyZMzFgwABoNBoUFRXZO6RO89NPP6G+vh6jRo2ydyh2p6CgABcuXDCe0AAwc+ZMzJs3z5ZxMazE/v37kZCQAJFIBI1GA5FIZO+QOk1SUhLu3buHv/3tb/YOxe4cOHAAx44d0ytjb+f/ChAIBPYOgWEj2Co3g8EjWEIzGDzCoRN6xYoVcHV1RVxcXKfrunv3Lp555hlwHAelUgkAeOWVV7Bly5ZO1/00mzZtgkQiAcdx4DgO7u7umDJlCm7cuKG3X3p6OkaOHAmxWAyRSITnnnsOn3/+ud4+WVlZ+MMf/gCpVIpu3brB09MTgwcPxvHjxy0eN2C9MbEHarUaiYmJiI6O1p2PTZs26bafPXsWffv2haurK+bPn2+zuJRKJYYMGQKRSASJRIKXXnoJ58+fx+HDh5GZmdm5yqkFUVFRlJaWRo7E3Llzac2aNRapq6ysjABQY2OjRepri+TkZJLL5aTRaOjWrVs0depUCggIILVaTUREiYmJJBQKaceOHfTo0SNSKBT05ZdfkqenJ61cuZKIiPbs2UOurq70ySef0K1bt+jx48dUXl5Oe/bsoa1bt7Ybg4+PDz18+NCq/TSH999/n0pKSjpVx6effkp/+ctf2t2vqamJZsyYQRcuXCCiJ+fDz8+PpFIpVVVV6fYrKSmhyMjITsVkLnV1dTRv3jx69OgRPXr0iP785z+Tv78/ERHFxsbSjh07TKpn//79NHfuXL0yh75DWxqO4yxST35+Pq5evWrSvk5OTnjmmWfw/vvvo6ioCLdu3UJtbS02bNiADz74AAsWLICHhwdEIhFCQkKQnJyMjRs3orCwEDExMVi1ahVWrFiBZ555Bq6urpDL5YiIiMDixYst0hdbcvDgQZu19eGHH6Jnz54YMWKErmz9+vUQCoXYsGGDzeIwhEQiQVpaGjw8PODh4YHQ0FCUlpaCiBAXF4f4+Hhcu3atQ3WbndDR0dHgOA6ZmZkICQlBbGwsjhw5gsDAQHh6emLBggVQqVStjgsNDQXHcSgqKsL9+/fh5+cHiUQCAIiJiQHHcZg4cSIkEgn8/f3x5ZdfGmz/xIkTGDZsGCQSCYKDg/WmnosWLYJUKoWbmxsiIiKg1Wpx/PhxDBs2DEKhEEOHDtXtu3PnTgiFQt10ftWqVeA4Dm+99RYGDRoEiUSC9evXAwCOHj2KoKAgCIVC+Pr64n/+538wcOBAs8atqakJANCtWzd8//33UCgUeO2111rtFxISAo1Gg/Xr1+PBgweYPXu2We10hpZj0tZ4tHW+jJ3nsLAwFBQUwNfXF0uXLgUATJ06FcuXL7d4PzQaDVJTUzFnzhy9cqlUiq1btyIlJQW3bt0yeKyh66utsQBg0vVvCK1Wi4qKCuzevRthYWHgOA5ubm6YMWMGtm3b1rHOt7xdmzrllsvllJ6eTjU1NbRs2TJyc3OjjIwMqqqqouHDh1NSUpLB4wBQYWEhERFdvnyZxGKxbptYLKasrCxqaGig1NRUEgqFVFZWpjflrqysJLFYTHv37qXa2lpKTU0lsVhMFRUVREQUHR1NZWVlVFhYSC4uLnTy5EkSCoX02WefUWNjIxUWFupNuSMjI/Wm83K5nM6cOUNarZZ27dpFIpGIlEolubu70969e0mhUFBMTAy9+OKL7Y5Ryyl3UVERTZw4kZ599lnSaDT0xRdfEABSKBQGj5VKpeTr60sASKlUttuWMToy5W45JobGoxlj54vI8HlWq9UEwCZT7vz8fAJAtbW1urLk5GTKyMggIqKwsDAKCQkhIv0pd1vXl7GxKCsrM/n6f5pJkyYRABo/frzuGiYi2rlzp24K3hYWnXL7+fnB09MTI0eOhK+vL1599VXIZDJMmzYNp0+f7lCdvXr1gpubG6KiotCjRw/k5ubqbc/OzoZcLsesWbPg7u6u269ZiJeUlAQfHx8EBARAJpPh0KFDkMvlWLJkCYRCoW5G0B4cx2HMmDFoaGhAcXEx6urqMH36dIhEIvzxj380+tf9aSoqKuDs7Izg4GA4OTnh8OHDcHJqf8iJSCfws9RjQmdoOR7NMw2g/fNlL4qLiyEQCODu7m5we3JyMk6dOoVz587plbd3fQGtxyI3N7fD1//Ro0dRXl6OCRMm4LnnnkN1dTWAJ9+Su3Pnjs4FZw6dfoaurKzEzZs3dSu669atQ01NDRISEnRlHMe1WuFtj549e6KqqkqvrKKiAt7e3nplcrkcFRUVqKurw8yZM+Hl5QVXV1dUVFSgrKwM/fr161T/fHx8IBQK8c0336CxsRFHjx41+dVYuVwOIkJ9fT2OHTumM4X4+voCAO7fv9/qmIaGBtTU1OD3v/89AHSZ1zUNnS970djY2ObLNN7e3khKSsKKFSv0ytu6voxh7Po3BWdnZ8jlcqxevRqurq749ttvATx5EUir1eo+jTGHTie0VCrF0KFDQUS6n+zsbMTFxemVBQUFmVwn/fubQU9/d9fHxweVlZV6ZeXl5fDx8cGePXtw/fp1/PDDD2hsbIRcLodUKm21v7lIJBIkJiZi4cKFkEqlOHfuHFJSUjpV5+9+9ztIJBIcOXKk1bbDhw+jW7duSEhIgLe3N5KTk1vto9FosHbt2k7FYEmMnS97IRKJ2n2OnTlzJuRyud5aTVvXlzGMXf/m0vJurFKp4OTkBKFQaHY9nU7osWPHoqCgAPv27YNCodDdYQwhkUiQl5cHtVqN0tLSVtvr6+uhVCqRnJwMlUql99I5AEyYMAG//PIL9u7di7q6OqSmpqK6uhoTJkzA48ePIRAIIJFIUFBQAKVSiZdffhk3b95Eeno66uvrcfToUbP719DQgIMHD+LKlStQKpU4d+5cp7+84u7ujg0bNmDdunXYuXMn6urq0NDQgMOHD+Odd95BbGwsfH19sWXLFuzYsQOxsbEoLi6GWq3G7du3sX79eqjV6k7FYAmMnS9D59nJyQlOTk64fv16K3GhpfHz84NSqUR9fX2b+6WkpOh95t7W9WUMc67/Zr755hts27YNSqUSjx49QlJSEiorK/Hyyy8DeHLX79+/f8cet1o+UJuyKBYdHU0AqFevXpSXl0dERAcOHKDAwEASCAQ0cuRIunjxosFj165dS0KhkAIDAykqKooA0Pz584noySKLTCYjFxcXCg4OptzcXHrvvffIxcWFRCIRffLJJ0RElJWVRUOHDiWRSETBwcGUnZ1NRER37tyhgQMHklgspvDwcBowYAANGDCAtmzZQn379iWpVEpvvvkmAaAZM2bQqlWryNXVlUQiEX388ce0cuVKAkD9+/enmpoaGjJkCAGgkJAQGjVqFAEgAMRxHD3zzDOUm5trdIx2795N7u7uBIACAgLoX//6l8H99u/fTyNHjiSRSEQcxxEAio+PJ61Wq9vn9OnTNHnyZOrevTs5OTmRVCqll156SbfA0xbmLoq1HJPm/j49HrNnzyYiw+erGWPnOTQ0lAQCAYWHhxMR0ZQpU+jdd981OT4i0xbFmpqaqHfv3pSfn09ERJs3byaJREIymYy2b9+ut29aWpre59CGri9j10bzWBi6/isrK8nb25uSk5NbxXfx4kXy9/cngUBAIpGIXnjhBcrJydFtX758uUnjYmhRzGFeLBGLxXT16lW7tN0WDx48oLlz55JKpSKiJxfL+++/T3/6058s2k5VVRUNHDiQ+vbtSydOnCCNRtPpOq35Yom9zpepL5YkJCRQTEyMDSIyjEqlojfeeIMSEhLMOk6tVlNAQABdu3at3X0d/sUSrVZr7xBakZOTg7t376KmpgYqlQoFBQU4ffo0vL299Rb9nv65d++eWe3IZDLk5OQgKCgIU6ZMwbRp06zUI8vhiOermdWrV+P27ds4f/68XdpPTU2Fl5cXYmJizDouLi4OsbGxGDRoUIfadYiEjoiIgEKhwOTJk3Hp0iV7h6PHlClTIJfLMXDgQIjFYkyaNAkvvfQSPvvsM72FkKd/+vbta3Zbffr0QXZ2NpRKJb777jsr9MYyOPL5asbZ2RmHDh1CTk6OXf7tT3R0NFJSUuDm5mbyMQcPHsSYMWM69V65Q3wfOj09Henp6fYOwyBisZipe57Ckc9XS1xcXBAbG2vvMEwmLCys03U4xB2awWBYBpbQDAaPaDXlbmxsRG1trT1iYVgYIkJdXR2cnZ3tHYrFUCqVUKlU7BoFdP8RpSV6Xu45c+bg66+/7tAbKgzHo7a2Fu7u7g7xPrilaGxshLOzM1xdXe0dit15/PgxJk6ciMOHD+vK9O7QIpEISUlJzPrJE3r16oVr167xSq7PrJ//wZD1kz1DMxg8giU0g8EjWEIzGDyiwwn9tNmyW7du8PX1xQcffACNRmP0uJUrV0IoFMLJyQmjR4/WlZ87dw6+vr5wcXFBRESE0eMd3QRqivHTEW2ffMRRrZ/NXL9+HePGjdM9B9vd+tms2SF6YjL8+uuvSSAQ0JYtW9o8btWqVTRy5MhW5ZWVlbpvsLSFo5tA2zJ+2sL22Yy1vpxhCXtnR+vgg/WTiGjfvn20evVqkslklJmZqSt3GOunRCLB9OnTMW7cOJw6dcpS1Voda5pAnzZ+Xr58mRe2T0vYO61tAHVk6ycAhIeH46OPPmr1rrfNrZ/tQUS6IDtqQwQsYwI1ZAEFYJIJtD3Tozkm0GYP14ULFxzO9mls7Ey1dzqiAbSrWD8NYRfrZzMtp9wKhYK+/fZbcnV1pe+++65NG6KpU+7OmkCftoBeuXKFKioqTDaBGjM9tmcCNWb8/Pzzz21i+2ymvSl3exZVmGjvtKUBlE/WTyKiPn366E25iexk/WymoqICHMdBIpHg9ddfx6effoopU6Z0yobYks6YQJ+2gNbV1SEzM9NsE+jTpseSkpJ2TaAdMX6SjW2fplguTcWRDKBdxfppDLtaP5vNltevX4eTkxNcXFwAtG1DdHZ2NujEUqlUugvaEOaYQIuKilpZQAHYzARqyPjpaLbPjlguTcHeBtCuYv00hl2tn80MHDgQa9euxdKlS3Hp0qU2bYh+fn64desWFAqFXh3nz583mmxkpgk0KyurlQUUgF1NoI5m++yI5bI9jJ0nW9IVrZ8tsav1syUrV65EYGAgQkJCEBwcbNSG+Kc//QkCgQAzZ85EXl4erl69it27d2PFihVYsGCBXp0dNYH+/ve/b2UBBWBXE6ij2T7bs1yaY+90JAOoo1s/28Om1s9mWpotAwMD6dKlS0REdOHCBXJ2diYvLy9atmyZURtoUVERRUZG0vDhw2ngwIE0Y8YMnaWxmc6YQI1ZQDUaDW3durVdEyjasF4qlUqjJlBTjJ/Wtn02Y8rn0MYsqkSm2zttaQDlg/WT6InZs3kBVCKR0KhRo6i0tFS3rctbPw3xazCBWsP22Yyt/p2sLc8Ts37+B4e3fhrCEc2Sxkyg//Vf/2V2XV3R9mkIRztPzPrpYDiyWdKYCXT16tUdqq+r2D4N4ajniVk/HQxHNksyE+h/cOTzxKyfDAajS8MSmsHgESyhGQwe0eoZOikpCV999ZU9YmFYGK1WizfffBNOTk4gIl7ofEtLS3WfLPzaKS0txW9/+1u9Mj2N77Vr11BSUmLzwBjW5eDBg1Cr1XjzzTftHQrDwvTp00cvqfXu0IMHD+70PzNnOB6XL1+GUqnEpEmT7B0Kw8qwZ2gGg0ewhGYweARLaAaDR7CEZjB4BEtoBoNHsIRmMHgES2gGg0ewhGYweARLaAaDR7CEZjB4BEtoBoNHsIRmMHgES2gGg0ewhGYweARLaAaDR7CEZjB4BEtoBoNHsIRmMHgES2gGg0ewhGYweARLaAaDR7CEZjB4BEtoBoNH6In2GfxAq9UiLi4ODQ0NAJ78AwWtVqsTsguFQvz1r3/lxX/SYOjDEpqn/Pd//zfOnDljcNuIESNw4cIFG0fEsAVsys1ToqKi4OHh0apcIpFg8eLFdoiIYQvYHZqnKBQK9OzZUzftbkYkEqG0tBTdu3e3U2QMaxvpmt8AABAsSURBVMLu0DxFLBZj3Lhx4DhOr3zUqFEsmXkMS2ges3DhQr1pt4eHB6KiouwYEcPasCk3j1Gr1ZDJZKivrwfw5K5dWVkJNzc3O0fGsBbsDs1jXFxcMH36dDg5OYHjOEyaNIklM89hCc1zIiMj4e7uDg8PDyxcuNDe4TCsDJty8xytVgsvLy9oNBpUVVWhW7du9g6JYUX0Ejo5ORlHjx61ZzwMC/Lo0SN4eHjgxo0b0Gg0ujfFujKPHz+GVqtljw7/ZuLEiVixYoXud70/11evXsWwYcMwefJkmwfGsDyvv/46du3ahfv370OtVvMioY8cOYLKykq2Wg/g5MmT+Omnn/TKWs2/goKCMHbsWFvFxLAirq6uGD16NKRSKYio1WfSXZEff/wRQqGQXaMAysvLcffuXb0ytij2K4EPycxoH5bQDAaPYAnNYPAIiyX04MGDwXEcHjx4YKkqLR7LihUr4Orqiri4OLvEZaj9V155BVu2bLFLPIZwtHg6g1qtRmJiIqKjoyGRSMBxHDZt2qTbfvbsWfTt2xeurq6YP3++zeO7fv06xo0bh2PHjgEADh8+jMzMzE7VabEPJb///ntIpVJLVdcpjMWyceNGVFVV2SEi4+139gRaGmvGs27dOixcuBB9+/a1WhvNaDQahIaGIjY2FiNGjMDAgQOxceNGJCQkYN68eZDJZBg9ejTy8/MRHx+P7du3Wz2mluzfvx8//vgj/u///k9XFhISgjVr1qCsrAwLFizoUL0Wn3K7uLhYusoO40ixMICDBw/arK0PP/wQPXv2xIgRI3Rl69evh1AoxIYNG2wWhzHCw8Px0Ucftfo8PS4uDvHx8bh27VqH6u1QQp84cQLDhg2DRCJBcHAwjh8/rts2dOhQCAQCBAQEYP/+/QAAlUqF0NBQiMVieHl5IS0tDcCTzxQDAwPh6emJBQsWQKVSITo6GhzHITMzEyEhIeA4DhzH4dlnnwUA5OTkoEePHujTpw8AYNGiRZBKpXBzc0NERAS0Wm2bsTyNoRiMxduSmJgYcByHiRMnQiKRwN/fH19++aVJY9TMzp07IRQKdVPwvLw8vPjiixCJRPD09MSaNWswZcoUcBwHPz8/3L9/H//85z/h6emJwYMHm3XOTKFlPKtWrQLHcXjrrbcwaNAgSCQSrF+/vt2+h4aGguM4FBUV4f79+/Dz84NEIkFYWBgKCgrg6+uLpUuXAgCmTp2K5cuXW7wfGo0GqampmDNnjl65VCrF1q1bkZKSglu3bhk81tB5a2ssAMPXUEdxc3PDjBkzsG3bto5VQC2IioqitLQ0aovKykoSi8W0d+9eqq2tpdTUVBKLxVRQUEAA6Mcff6TGxkbatm0bubi40M8//0wHDhygSZMmUUNDA125coX+/ve/U1lZGbm5uVFGRgZVVVXR8OHDKSkpiYiI5HI5paenU01NDS1atIicnJzoxx9/1MXw7rvv0k8//URERNHR0VRWVkaFhYXk4uJCV65coerqaqOxzJ07l9asWUNEZDQGQ/EaQiwWU1ZWFjU0NFBqaioJhUIqKyszOkYVFRV67RMRRUZG0po1a+jBgwcklUpp48aNpFAo6O7du/Tee++RQqEgT09P+vrrr/XOU1lZWZvniYjIx8eHHj582O5+LWmOp/k8nDlzhrRaLe3atYtEIlG7fSciAkCFhYVERHT58mUSi8WkVqsJAJWUlJgVz9N8+umn9Je//KXNffLz8wkA1dbW6sqSk5MpIyODiIjCwsIoJCSEiIhKSkooMjKSiIxf2xUVFUbHoq3r2BT69OlDmZmZemU7d+4kf3//do/dv38/zZ07V6/M7Dt0dnY25HI5Zs2aBXd3d0RFRaFHjx44efIkAKBXr14QCoW6Z6VTp05BIpHg0qVLyMrKQlBQEN577z3k5ubC19cXr776KmQyGaZNm4bTp0/r2vHz84Onpyc+//xzTJs2DUlJSQCAhoYG3Lt3T/fWU1JSEnx8fBAQEACZTIa6ujpdHYZiaYmxGAzFa4xevXrBzc1NNw65ubntjpEhcnJyIBKJEBMTA5FIBF9fX/z973+HSCRCeHg49u3bB+DJQo9arYaPj4+ZZ67jcByHMWPGoKGhAU1NTbpyQ313BIqLiyEQCODu7m5we3JyMk6dOoVz587plZty3p4ei/au447g5eWFO3fugDrwNQuzE7qiogLe3t56ZXK5HBUVFa327dGjB2pqavDHP/4Ry5Ytw+LFi+Hn54cTJ06gsrISN2/e1E2p161bh5qaGoNtLlu2DHv37kVVVRX27duHiIgIAEBdXR1mzpwJLy8vuLq6Gozh6VhaYiwGQ/EmJCTo9uM4Djdu3GjVRs+ePVFVVWXWGDVz//593WPE00RGRiIjIwN1dXXIysrCa6+9ZrQee9Hcd0egsbERAoHA6HZvb28kJSXpvQMNmHdtN2POdWwqAoEAWq0WSqXS7GPNTmgfHx9UVlbqlZWXl7e6YxARSkpK4OvrC47jEBsbi3v37uHPf/4zli5dCqlUiqFDh4KIdD/Z2dkG2xwzZgwGDRqEL774AseOHcPUqVMBAHv27MH169fxww8/oLGxEXK53ODxLWNpibEYDMUbFxent19QUFCrNkpLS9GnTx+Tx6glcrkcZWVlBrc9//zzCAwMxFdffYW8vDyHe9e+Zd8dAZFI1O5z7MyZMyGXy/XWPTpy3sy5jk1FpVLByckJQqHQ7GPNTugJEybgl19+wd69e1FXV4fU1FRUV1djwoQJAJ78dVQqlUhKSoJarcbEiRPxxRdfIDs7GxqNBiNGjADHcRg7diwKCgqwb98+KBQKNDQ0tPmXbdmyZfjwww/x4osvwsnpSdiPHz+GQCCARCJBQUFBq79ohmJpibEYDMVrjPr6eiiVSiQnJ0OlUmHcuHHtjpEh/vCHP6C6uhrr16/HgwcPoFar9RI8MjISaWlpkMlkDuPTNtR34IlZNC8vD2q1GqWlpQAAJycnODk54fr1663EhZbGz88PSqVSZ2oxRkpKit5n7h05b+Zex6ZQWVmJ/v37d+x13ZYP1KYsihERZWVl0dChQ0kkElFwcDBlZ2dTY2MjTZo0iXr06EECgYCGDx9OeXl5RESUkZFBvXv3pm7dulFgYCBlZ2cTEdGBAwcoMDCQBAIBjRw5ki5evEjR0dEEgHr16qU7nojo8ePHNGDAAL1Fnjt37tDAgQNJLBZTeHg4DRgwgAYMGEAKhcJgLO+99x65uLiQSCSiTz75xGgMxuJ9GrFYTDKZjFxcXCg4OJhyc3PbHKOn21+1ahW5urqSSCSijz/+mP71r3/R888/TyKRiPr06UObN2/W1ffw4UMSiURUXFzc7vlpxtxFsZbxACAA1L9/f6qpqaEhQ4YQAJo9e3a7fV+7di0JhUIKDAykqKgoAkDz58+n0NBQEggEFB4eTkREU6ZMoXfffdfk+IhMWxRramqi3r17U35+PhERbd68mSQSCclkMtq+fbvevmlpabpFMSLD523lypVtjoWha6iyspK8vb0pOTnZYIzLly8nX19fAkASiYRGjRpFpaWlum2mjIuhRbEOJTTjCWKxmK5evWqTtrRaLb3zzjtmHdORVW5TsWXfW2JKQhMRJSQkUExMjA0iMoxKpaI33niDEhISzDpOrVZTQEAAXbt2rd19LbLKzdCn5efe1uDMmTNQKBSIj4/H9OnTrdqWuVi7751h9erVuH37Ns6fP2+X9lNTU+Hl5YWYmBizjouLi0NsbCwGDRrUoXZZQneQiIgIKBQKTJ48GZcuXbJaOykpKfDx8QHHcbpnVHtjq753BmdnZxw6dAg5OTkoLi62efvR0dFISUkxy6xy8OBBjBkzplPvlTPBVAdJT09Henq61dsx9oabPbFV3zuLi4sLYmNj7R2GyYSFhXW6DnaHZjB4BEtoBoNH6E25m5qacO7cOaZ65QlKpRKHDh2CWCy2dygW4+LFi6iursY//vEPe4did/Lz8/VexQWeSmiNRoPCwkJoNBqbBsawDk1NTTh79mybr0F2NQoLC6FUKh3mvXF7cvv2bfTs2VOvTC+hBQIB5s+fj3nz5tkyLoaVOHr0KJKSkhxGPGEJkpKScO/ePfztb3+zdyh258CBAzrbSTPsGZrB4BEsoRkMHsESmsHgEQ6V0IWFhZg6dSp69OgBoVCIfv364dChQ/YOqxXtmTHtbRdlPMFRrZ+1tbV49tlnIZFIIJVK8corr6CoqMgi1k+HSuiwsDD07t0bN27cQFVVFZKSknD//n17h4V169bh3r17ut8zMzOxZMkSo/tv3LgRs2bNskVoduHp8bBXHW3RbP0cP348kpOTkZiYCD8/PyQkJODhw4cAoLN+zpkzx6A3zlqoVCr87ne/Q1lZGW7fvo3u3bvjzTffREhICM6ePYudO3d2uG6HSWi1Wo0ffvgBq1evhre3N8RiMWbMmIFly5aZVU9lZWWH1C1tYUtbZVfAEuNh7TF1ZOunl5cXUlNT4e7uDqlUigULFuB///d/odFo7GP9tAYuLi4IDAzUM0i0xJhZMSsrC8HBwRAKhejVqxd69+6N6dOnGzRPtlWXMbPj07bKp02dbVlHuwLG7KSm2jsd0QDa1ayfCoUCPXr0gLOzs+2tn9YkPz+f+vbtS6NHj6YdO3ZQfX09ERk3K5aXl5Obmxt99tln1NjYSDdv3iQA1NjYaNA82VZdRIYtl4ZslS3NmIaso0TUyu5pD9r7PnRblksi0+2dtjSA8s36SUQUExND0dHRut9tav20JiNHjkRRURHeeust7NixA4GBgbh06ZJRs2JmZiZ69uyJJUuWQCgUGrU8tsQUS6Mxy6Uh2rKOOjodsZMaw5EMoF3J+llSUoJjx47hgw8+0JXZ1PppbQQCAWbNmqWT4cXExBg1K5aXl6Nfv35m1W9JS6M51lFHpCOWS1OwtwG0q1g/6+rqsHDhQhw+fFjvbT6bWj+tRX19vd5zCQBMnz4dJSUlRs2KMpnM7H+OZ0lLo6nWUUelI5bL9iAHMIB2BetnXV0dFixYgM2bN7eyk9jU+mlNdu/ejZycHCiVSty5cwdbtmzB2LFjjZoVx48fj4KCAuzduxcqlQrl5eW6ugyZJwHzLY1t2Srbs446Ou1ZLs2xdzqSAdTRrZ+1tbWIjIzERx99ZFA1ZHPrpzVQqVQ0e/Zs8vX1pW7dupGPjw/Nnz9ft6hjyKxIRLRt2zbq168fubi46CyKjY2NRs2Txupqy+zY0lbZ0oy5ZMkSg9bRmJiYVnZRe2CKJNCQ5bIZU+2dtjSA8sH6uX37dp1VteXPmTNniIhZP3WUl5frEpphXetnS2xpAGXWz//g8KvcnYUs/EIJw3Qc7fN3Zv3kAc3POosXL7ZzJL8eHNUAyqyfPODKlSv2DuFXhyMbQJn1k8FgdGlYQjMYPIIlNIPBIzhqsTQcHR2NL774wp7xMCyIRqNxmH89ayno329jNf9L4V87ERERejmrl9AMBqNrw/7MMRg8giU0g8EjugG4be8gGAyGZfh/cgbehFAcnrAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec-s2ECYwpUS"
      },
      "source": [
        "You can now train the PQC policy on CartPole-v1, using, e.g., the basic `REINFORCE` algorithm (see Alg. 1 in <a href=\"https://arxiv.org/abs/2103.05577\" class=\"external\">[1]</a>). Pay attention to the following points:\n",
        "1. Because scaling parameters, variational angles and observables weights are trained with different learning rates, it is convenient to define 3 separate optimizers with their own learning rates, each updating one of these groups of parameters.\n",
        "2. The loss function in policy-gradient RL is\n",
        "    $$ \\mathcal{L}(\\theta) = -\\frac{1}{|\\mathcal{B}|}\\sum_{s_0,a_0,r_1,s_1,a_1, \\ldots \\in \\mathcal{B}} \\left(\\sum_{t=0}^{H-1} \\log(\\pi_\\theta(a_t|s_t)) \\sum_{t'=1}^{H-t} \\gamma^{t'} r_{t+t'} \\right)$$\n",
        "for a batch $\\mathcal{B}$ of episodes $(s_0,a_0,r_1,s_1,a_1, \\ldots)$ of interactions in the environment following the policy $\\pi_\\theta$. This is different from a supervised learning loss with fixed target values that the model should fit, which make it impossible to use a simple function call like `model.fit` to train the policy. Instead, using a `tf.GradientTape` allows to keep track of the computations involving the PQC (i.e., policy sampling) and store their contributions to the loss during the interaction. After running a batch of episodes, you can then apply backpropagation on these computations to get the gradients of the loss with respect to the PQC parameters and use the optimizers to update the policy-model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHS7UlTHwpUS"
      },
      "source": [
        "Start by defining a function that gathers episodes of interaction with the environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dYepv83JwpUT"
      },
      "outputs": [],
      "source": [
        "def gather_episodes(state_bounds, n_actions, model, n_episodes, env_name):\n",
        "    \"\"\"Interact with environment in batched fashion.\"\"\"\n",
        "\n",
        "    trajectories = [defaultdict(list) for _ in range(n_episodes)]\n",
        "    envs = [gym.make(env_name) for _ in range(n_episodes)]\n",
        "\n",
        "    done = [False for _ in range(n_episodes)]\n",
        "    states = [e.reset() for e in envs]\n",
        "\n",
        "    while not all(done):\n",
        "        unfinished_ids = [i for i in range(n_episodes) if not done[i]]\n",
        "        normalized_states = [s/state_bounds for i, s in enumerate(states) if not done[i]]\n",
        "\n",
        "        for i, state in zip(unfinished_ids, normalized_states):\n",
        "            trajectories[i]['states'].append(state)\n",
        "\n",
        "        # Compute policy for all unfinished envs in parallel\n",
        "        states = tf.convert_to_tensor(normalized_states)\n",
        "        action_probs = model([states])\n",
        "\n",
        "        # Store action and transition all environments to the next state\n",
        "        states = [None for i in range(n_episodes)]\n",
        "        for i, policy in zip(unfinished_ids, action_probs.numpy()):\n",
        "            action = np.random.choice(n_actions, p=policy)\n",
        "            states[i], reward, done[i], _ = envs[i].step(action)\n",
        "            trajectories[i]['actions'].append(action)\n",
        "            trajectories[i]['rewards'].append(reward)\n",
        "\n",
        "    return trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJRGD1g1wpUT"
      },
      "source": [
        "and a function that computes discounted returns $\\sum_{t'=1}^{H-t} \\gamma^{t'} r_{t+t'}$ out of the rewards $r_t$ collected in an episode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KGDLrNN1wpUT"
      },
      "outputs": [],
      "source": [
        "def compute_returns(rewards_history, gamma):\n",
        "    \"\"\"Compute discounted returns with discount factor `gamma`.\"\"\"\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards_history[::-1]:\n",
        "        discounted_sum = r + gamma * discounted_sum\n",
        "        returns.insert(0, discounted_sum)\n",
        "\n",
        "    # Normalize them for faster and more stable learning\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
        "    returns = returns.tolist()\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkuUMdskwpUT"
      },
      "source": [
        "Define the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QUuSU1LRwpUU"
      },
      "outputs": [],
      "source": [
        "state_bounds = np.array([-np.inf, np.inf ])\n",
        "gamma = 1\n",
        "batch_size = 10\n",
        "n_episodes = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM8uFSLMwpUU"
      },
      "source": [
        "Prepare the optimizers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2fxGvCKpwpUU"
      },
      "outputs": [],
      "source": [
        "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True)\n",
        "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "\n",
        "# Assign the model parameters to each optimizer\n",
        "w_in, w_var, w_out = 1, 0, 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbVHz19-wpUU"
      },
      "source": [
        "Implement a function that updates the policy using states, actions and returns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zLfbu8Q2wpUV"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def reinforce_update(states, actions, returns, model):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    returns = tf.convert_to_tensor(returns)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        logits = model(states)\n",
        "        p_actions = tf.gather_nd(logits, actions)\n",
        "        log_probs = tf.math.log(p_actions)\n",
        "        loss = tf.math.reduce_sum(-log_probs * returns) / batch_size\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
        "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrPlDlqLwpUV"
      },
      "source": [
        "Now implement the main training loop of the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Foz1XewpUV"
      },
      "source": [
        "Note: This agent may need to simulate several million quantum circuits and can take as much as ~20 minutes to finish training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cYSDSNGlwpUW",
        "outputId": "2cd29fd4-07c1-4877-e108-b30b99ab29ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 3), found shape=(10, 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-3d89d888235a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Gather episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgather_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Group states, actions and returns in numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-45fd9c8d80f8>\u001b[0m in \u001b[0;36mgather_episodes\u001b[0;34m(state_bounds, n_actions, model, n_episodes, env_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Compute policy for all unfinished envs in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Store action and transition all environments to the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    299\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"model_5\" is incompatible with the layer: expected shape=(None, 3), found shape=(10, 2)"
          ]
        }
      ],
      "source": [
        "env_name = \"MountainCar-v0\"\n",
        "\n",
        "# Start training the agent\n",
        "episode_reward_history = []\n",
        "for batch in range(n_episodes // batch_size):\n",
        "    # Gather episodes\n",
        "    episodes = gather_episodes(state_bounds, n_actions, model, batch_size, env_name)\n",
        "\n",
        "    # Group states, actions and returns in numpy arrays\n",
        "    states = np.concatenate([ep['states'] for ep in episodes])\n",
        "    actions = np.concatenate([ep['actions'] for ep in episodes])\n",
        "    rewards = [ep['rewards'] for ep in episodes]\n",
        "    returns = np.concatenate([compute_returns(ep_rwds, gamma) for ep_rwds in rewards])\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "\n",
        "    id_action_pairs = np.array([[i, a] for i, a in enumerate(actions)])\n",
        "\n",
        "    # Update model parameters.\n",
        "    reinforce_update(states, id_action_pairs, returns, model)\n",
        "\n",
        "    # Store collected rewards\n",
        "    for ep_rwds in rewards:\n",
        "        episode_reward_history.append(np.sum(ep_rwds))\n",
        "\n",
        "    avg_rewards = np.mean(episode_reward_history[-10:])\n",
        "\n",
        "    print('Finished episode', (batch + 1) * batch_size,\n",
        "          'Average rewards: ', avg_rewards)\n",
        "\n",
        "    if avg_rewards >= 500.0:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E7Be2SqwpUW"
      },
      "source": [
        "Plot the learning history of the agent:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define environmen\n",
        "env = gym.make('MountainCar')\n",
        "\n",
        "# Hyperparameters\n",
        "n_qubits = 2  # Number of qubits for the PQC\n",
        "n_layers = 3  # Number of layers in the PQC\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "episodes = 1000\n",
        "\n",
        "# Function for a single qubit rotation around a specific axis\n",
        "def one_qubit_rotation(qubit, symbol):\n",
        "  \"\"\"\n",
        "  Applies a rotation around a specific axis (X, Y or Z) on the given qubit.\n",
        "  \"\"\"\n",
        "  return [cirq.rx(symbol[0])(qubit), cirq.ry(symbol[1])(qubit), cirq.rz(symbol[2])(qubit)]\n",
        "\n",
        "# Function to create an entangling layer with CZ gates\n",
        "def entangling_layer(qubits):\n",
        "  \"\"\"\n",
        "  Creates a layer of CZ entangling gates on a circular topology of qubits.\n",
        "  \"\"\"\n",
        "  cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
        "  cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
        "  return cz_ops\n",
        "\n",
        "# Function to generate the PQC circuit\n",
        "def generate_circuit(qubits, n_layers):\n",
        "  \"\"\"\n",
        "  Generates a quantum circuit with variational and entangling layers.\n",
        "  \"\"\"\n",
        "  circuit = cirq.Circuit()\n",
        "  symbols = tf.Variable(list(sympy.symbols('theta_' + str(i) + '_' + str(j) for i in range(n_layers+1) for j in range(3))), dtype=tf.float32)\n",
        "\n",
        "\n",
        "  for layer in range(n_layers):\n",
        "    # Variational layer (rotations on each qubit)\n",
        "    circuit += cirq.Circuit(one_qubit_rotation(q, symbols[layer, i]) for i, q in enumerate(qubits))\n",
        "    circuit += entangling_layer(qubits)\n",
        "\n",
        "  # Final variational layer\n",
        "  circuit += cirq.Circuit(one_qubit_rotation(q, symbols[n_layers, i]) for i, q in enumerate(qubits))\n",
        "\n",
        "  return circuit, symbols\n",
        "\n",
        "# Class for the PQC policy model (maps state to action probabilities)\n",
        "class PQCPolicy(keras.Model):\n",
        "  def __init__(self, qubits, n_layers):\n",
        "    super(PQCPolicy, self).__init__()\n",
        "    self.qubits = qubits\n",
        "    self.n_layers = n_layers\n",
        "    self.circuit, self.symbols = generate_circuit(qubits, n_layers)\n",
        "\n",
        "  def call(self, state):\n",
        "    # Convert state to a bitstring for the PQC input\n",
        "    state_bin = format(int(state * (2**16)), '016b')\n",
        "    state_circuit = cirq.Circuit(cirq.rx(float(c))[::-1](q) for c, q in zip(state_bin, self.qubits))\n",
        "    circuit = cirq.Circuit.compose(self.circuit, state_circuit)\n",
        "\n",
        "    # Simulate the circuit and get measurement probabilities\n",
        "    simulator = cirq.Simulator()\n",
        "    results = simulator.run(circuit, repetitions=100)\n",
        "    measurement = results.measurements['m'][:, -1]\n",
        "    action_probs = np.mean(measurement, axis=0)\n",
        "\n",
        "    return action_probs\n",
        "\n",
        "# Function to perform an action in the environment\n",
        "def interact_with_env(state, policy_model):\n",
        "  action_probs = policy_model(state)\n",
        "  action = np.random.choice(env.action_space.n, p=action_probs)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  return next_state, reward, done\n",
        "\n",
        "# Function to calculate the discounted reward sum\n",
        "def discounted_reward_sum(rewards, gamma):\n",
        "  discounted_sum = 0\n",
        "  for r in rewards[::-1]:\n",
        "    discounted_sum = r + gamma * discounted_sum\n",
        "  return discounted_sum"
      ],
      "metadata": {
        "id": "j0AFWnCG2gzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy gradient update step (continued)\n",
        "def policy_gradient_update(policy_model, episodes, gamma, learning_rate):\n",
        "  \"\"\"\n",
        "  Performs a policy gradient update based on rewards collected in an episode.\n",
        "  \"\"\"\n",
        "  for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    rewards, states, actions = [], [], []\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      # Get action probabilities and sample an action\n",
        "      action_probs = policy_model(state)\n",
        "      action = np.random.choice(env.action_space.n, p=action_probs)\n",
        "\n",
        "      # Step in the environment\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      # Store experience\n",
        "      rewards.append(reward)\n",
        "      states.append(state)\n",
        "      actions.append(action)\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "    # Calculate discounted reward sum for the episode\n",
        "    discounted_rewards = discounted_reward_sum(rewards, gamma)\n",
        "\n",
        "    # Policy gradient loss (advantage * log probability)\n",
        "    with tf.GradientTape() as tape:\n",
        "      log_probs = tf.math.log(policy_model(state))\n",
        "      policy_loss = -tf.reduce_sum(log_probs[actions] * discounted_rewards)\n",
        "\n",
        "    # Update policy model parameters\n",
        "    grads = tape.gradient(policy_loss, policy_model.trainable_variables)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "    optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n",
        "\n",
        "# Create the PQC policy model\n",
        "policy_model = PQCPolicy(n_qubits, n_layers)\n",
        "\n",
        "# Train the policy model using policy gradients\n",
        "policy_gradient_update(policy_model, episodes, gamma, learning_rate)\n",
        "\n",
        "# Evaluate the trained agent\n",
        "total_reward = 0\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  action_probs = policy_model(state)\n",
        "  action = np.random.choice(env.action_space.n, p=action_probs)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  total_reward += reward\n",
        "  state = next_state\n",
        "\n",
        "env.close()\n",
        "print(\"Total reward in the evaluation episode:\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EHwGGQ0C2gvu",
        "outputId": "f0032d6d-689f-4559-b32b-61aabc00ce2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot create 'generator' instances",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-135b7344b14f>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Create the PQC policy model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mpolicy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPQCPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_qubits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Train the policy model using policy gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-b29b7e1ef970>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, qubits, n_layers)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqubits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqubits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_circuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqubits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-b29b7e1ef970>\u001b[0m in \u001b[0;36mgenerate_circuit\u001b[0;34m(qubits, n_layers)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \"\"\"\n\u001b[1;32m     37\u001b[0m   \u001b[0mcircuit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcirq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCircuit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'theta_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sympy/core/symbol.py\u001b[0m in \u001b[0;36msymbols\u001b[0;34m(names, cls, **args)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot create 'generator' instances"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9uWD7-S2gsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51RzNBZqwpUX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_reward_history)\n",
        "plt.xlabel('Epsiode')\n",
        "plt.ylabel('Collected rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfdf4DBkwpUX"
      },
      "source": [
        "Congratulations, you have trained a quantum policy gradient model on Cartpole! The plot above shows the rewards collected by the agent per episode throughout its interaction with the environment. You should see that after a few hundred episodes, the performance of the agent gets close to optimal, i.e., 500 rewards per episode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtaBfoERwpUX"
      },
      "source": [
        "You can now visualize the performance of your agent using `env.render()` in a sample episode (uncomment/run the following cell only if your notebook has access to a display):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VpROTJ1wpUX"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "\n",
        "# env = gym.make('CartPole-v1')\n",
        "# state = env.reset()\n",
        "# frames = []\n",
        "# for t in range(500):\n",
        "#     im = Image.fromarray(env.render(mode='rgb_array'))\n",
        "#     frames.append(im)\n",
        "#     policy = model([tf.convert_to_tensor([state/state_bounds])])\n",
        "#     action = np.random.choice(n_actions, p=policy.numpy()[0])\n",
        "#     state, _, done, _ = env.step(action)\n",
        "#     if done:\n",
        "#         break\n",
        "# env.close()\n",
        "# frames[1].save('./images/gym_CartPole.gif',\n",
        "#                save_all=True, append_images=frames[2:], optimize=False, duration=40, loop=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0iA0nubwpUX"
      },
      "source": [
        "<img src=\"https://github.com/tensorflow/quantum/blob/master/docs/tutorials/images/gym_CartPole.gif?raw=1\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAO1TBxqwpUX"
      },
      "source": [
        "## 3. Deep Q-learning with PQC Q-function approximators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uEimdpHwpUY"
      },
      "source": [
        "In this section, you will move to the implementation of the deep Q-learning algorithm presented in <a href=\"https://arxiv.org/abs/2103.15084\" class=\"external\">[2]</a>. As opposed to a policy-gradient approach, the deep Q-learning method uses a PQC to approximate the Q-function of the agent. That is, the PQC defines a function approximator:\n",
        "$$ Q_\\theta(s,a) = \\langle O_a \\rangle_{s,\\theta} $$\n",
        "where $\\langle O_a \\rangle_{s,\\theta}$ are expectation values of observables $O_a$ (one per action) measured at the ouput of the PQC.\n",
        "\n",
        "These Q-values are updated using a loss function derived from Q-learning:\n",
        "$$ \\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{s,a,r,s' \\in \\mathcal{B}} \\left(Q_\\theta(s,a) - [r +\\max_{a'} Q_{\\theta'}(s',a')]\\right)^2$$\n",
        "for a batch $\\mathcal{B}$ of $1$-step interactions $(s,a,r,s')$ with the environment, sampled from the replay memory, and parameters $\\theta'$ specifying the target PQC (i.e., a copy of the main PQC, whose parameters are sporadically copied from the main PQC throughout learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTyRzuDYwpUY"
      },
      "source": [
        "You can adopt the same observables used in <a href=\"https://arxiv.org/abs/2103.15084\" class=\"external\">[2]</a> for CartPole, namely a $Z_0Z_1$ Pauli product for action $0$ and a $Z_2Z_3$ Pauli product for action $1$. Both observables are re-scaled so their expectation values are in $[0,1]$ and weighted by an action-specific weight. To implement the re-scaling and weighting of the Pauli products, you can define again an extra `tf.keras.layers.Layer` that stores the action-specific weights and applies them multiplicatively on the expectation values $\\left(1+\\langle Z_0Z_1 \\rangle_{s,\\theta}\\right)/2$ and $\\left(1+\\langle Z_2Z_3 \\rangle_{s,\\theta}\\right)/2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX5l96qywpUY"
      },
      "outputs": [],
      "source": [
        "class Rescaling(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Rescaling, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.ones(shape=(1,input_dim)), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.multiply((inputs+1)/2, tf.repeat(self.w,repeats=tf.shape(inputs)[0],axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oesnEQa7wpUY"
      },
      "source": [
        "Prepare the definition of your PQC and its observables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpV0PxZqwpUY"
      },
      "outputs": [],
      "source": [
        "n_qubits = 2 # Dimension of the state vectors in CartPole\n",
        "n_layers = 5 # Number of layers in the PQC\n",
        "n_actions = 3 # Number of actions in CartPole\n",
        "\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "ops = [cirq.Z(q) for q in qubits]\n",
        "observables = [ops[0]*ops[1], ops[2]*ops[3]] # Z_0*Z_1 for action 0 and Z_2*Z_3 for action 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLMvQBXFwpUZ"
      },
      "source": [
        "Define a `tf.keras.Model` that, similarly to the PQC-policy model, constructs a Q-function approximator that is used to generate the main and target models of our Q-learning agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBGM6RHIwpUZ"
      },
      "outputs": [],
      "source": [
        "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
        "    \"\"\"Generates a Keras model for a data re-uploading PQC Q-function approximator.\"\"\"\n",
        "\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
        "    process = tf.keras.Sequential([Rescaling(len(observables))], name=target*\"Target\"+\"Q-values\")\n",
        "    Q_values = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = generate_model_Qlearning(qubits, n_layers, n_actions, observables, False)\n",
        "model_target = generate_model_Qlearning(qubits, n_layers, n_actions, observables, True)\n",
        "\n",
        "model_target.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57TxgIN5wpUZ"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHp42R4twpUa"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model_target, show_shapes=True, dpi=70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vricJOvXwpUa"
      },
      "source": [
        "You can now implement the deep Q-learning algorithm and test it on the CartPole-v1 environment. For the policy of the agent, you can use an $\\varepsilon$-greedy policy:\n",
        "$$ \\pi(a|s) =\n",
        "\\begin{cases}\n",
        "\\delta_{a,\\text{argmax}_{a'} Q_\\theta(s,a')}\\quad \\text{w.p.}\\quad 1 - \\varepsilon\\\\\n",
        "\\frac{1}{\\text{num_actions}}\\quad \\quad \\quad \\quad \\text{w.p.}\\quad \\varepsilon\n",
        "\\end{cases} $$\n",
        "where $\\varepsilon$ is multiplicatively decayed at each episode of interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMteuedswpUb"
      },
      "source": [
        "Start by defining a function that performs an interaction step in the environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L9cV26PwpUb"
      },
      "outputs": [],
      "source": [
        "def interact_env(state, model, epsilon, n_actions, env):\n",
        "    # Preprocess state\n",
        "    state_array = np.array(state)\n",
        "    state = tf.convert_to_tensor([state_array])\n",
        "\n",
        "    # Sample action\n",
        "    coin = np.random.random()\n",
        "    if coin > epsilon:\n",
        "        q_vals = model([state])\n",
        "        action = int(tf.argmax(q_vals[0]).numpy())\n",
        "    else:\n",
        "        action = np.random.choice(n_actions)\n",
        "\n",
        "    # Apply sampled action in the environment, receive reward and next state\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    interaction = {'state': state_array, 'action': action, 'next_state': next_state.copy(),\n",
        "                   'reward': reward, 'done':np.float32(done)}\n",
        "\n",
        "    return interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDiw3iJywpUb"
      },
      "source": [
        "and a function that updates the Q-function using a batch of interactions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR2DjesVwpUb"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def Q_learning_update(states, actions, rewards, next_states, done, model, gamma, n_actions):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    rewards = tf.convert_to_tensor(rewards)\n",
        "    next_states = tf.convert_to_tensor(next_states)\n",
        "    done = tf.convert_to_tensor(done)\n",
        "\n",
        "    # Compute their target q_values and the masks on sampled actions\n",
        "    future_rewards = model_target([next_states])\n",
        "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "                                                   * (1.0 - done))\n",
        "    masks = tf.one_hot(actions, n_actions)\n",
        "\n",
        "    # Train the model on the states and target Q-values\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        q_values = model([states])\n",
        "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
        "\n",
        "    # Backpropagation\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
        "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfXHhqaPwpUb"
      },
      "source": [
        "Define the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ937aYPwpUc"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "n_episodes = 2000\n",
        "\n",
        "# Define replay memory\n",
        "max_memory_length = 10000 # Maximum replay length\n",
        "replay_memory = deque(maxlen=max_memory_length)\n",
        "\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
        "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
        "batch_size = 16\n",
        "steps_per_update = 10 # Train the model every x steps\n",
        "steps_per_target_update = 30 # Update the target model every x steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHsHnuHmwpUc"
      },
      "source": [
        "Prepare the optimizers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "713nl3oUwpUc"
      },
      "outputs": [],
      "source": [
        "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "\n",
        "# Assign the model parameters to each optimizer\n",
        "w_in, w_var, w_out = 1, 0, 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwE0buDowpUd"
      },
      "source": [
        "Now implement the main training loop of the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjjTamvywpUd"
      },
      "source": [
        "Note: This agent may need to simulate several million quantum circuits and can take as much as ~40 minutes to finish training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er9fXHH_wpUd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "episode_reward_history = []\n",
        "step_count = 0\n",
        "for episode in range(n_episodes):\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        # Interact with env\n",
        "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
        "\n",
        "        # Store interaction in the replay memory\n",
        "        replay_memory.append(interaction)\n",
        "\n",
        "        state = interaction['next_state']\n",
        "        episode_reward += interaction['reward']\n",
        "        step_count += 1\n",
        "\n",
        "        # Update model\n",
        "        if step_count % steps_per_update == 0:\n",
        "            # Sample a batch of interactions and update Q_function\n",
        "            training_batch = np.random.choice(replay_memory, size=batch_size)\n",
        "            Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
        "                              np.asarray([x['action'] for x in training_batch]),\n",
        "                              np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
        "                              np.asarray([x['next_state'] for x in training_batch]),\n",
        "                              np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
        "                              model, gamma, n_actions)\n",
        "\n",
        "        # Update target model\n",
        "        if step_count % steps_per_target_update == 0:\n",
        "            model_target.set_weights(model.get_weights())\n",
        "\n",
        "        # Check if the episode is finished\n",
        "        if interaction['done']:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if (episode+1)%10 == 0:\n",
        "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
        "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
        "            episode+1, n_episodes, avg_rewards))\n",
        "        if avg_rewards >= 500.0:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG8BWOSYwpUd"
      },
      "source": [
        "Plot the learning history of the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSRMtk-swpUe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(episode_reward_history)\n",
        "plt.xlabel('Epsiode')\n",
        "plt.ylabel('Collected rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_7rJf0iwpUe"
      },
      "source": [
        "Similarly to the plot above, you should see that after ~1000 episodes, the performance of the agent gets close to optimal, i.e., 500 rewards per episode. Learning takes longer for Q-learning agents since the Q-function is a \"richer\" function to be learned than the policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8X49f8owpUe"
      },
      "source": [
        "## 4. Exercise\n",
        "\n",
        "Now that you have trained two different types of models, try experimenting with different environments (and different numbers of qubits and layers). You could also try combining the PQC models of the last two sections into an [actor-critic agent](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "X8X49f8owpUe"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (main, Dec  7 2022, 13:47:07) [GCC 12.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}