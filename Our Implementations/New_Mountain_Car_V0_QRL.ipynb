{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSoeBdTwpUF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPTH8ScrwpUG",
        "outputId": "066a34ad-36c1-4136-fc4f-33ead109d985"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZeJimx6wpUI",
        "outputId": "7a8e2973-8dca-400c-8033-84509604574b"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-quantum==0.7.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A2JRKhMwpUJ",
        "outputId": "61ea796b-7070-4cbf-a9f7-f6e19aee375b"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ql5PW-ACO0J",
        "outputId": "96e71235-c765-4827-9edf-1f5172839776"
      },
      "outputs": [],
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l25Gykt4qg96"
      },
      "source": [
        "## 1. Policy Gradient RL with PQC Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "kWhDct2Kl5P6",
        "outputId": "93a1dd60-106e-44fd-fb94-ee531a160fd0"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import cirq\n",
        "import sympy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "from collections import defaultdict\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Functions provided by you\n",
        "def one_qubit_rotation(qubit, symbols):\n",
        "    return [cirq.rx(symbols[0])(qubit), cirq.ry(symbols[1])(qubit), cirq.rz(symbols[2])(qubit)]\n",
        "\n",
        "def entangling_layer(qubits):\n",
        "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
        "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
        "    return cz_ops\n",
        "\n",
        "def generate_circuit(qubits, n_layers):\n",
        "    n_qubits = len(qubits)\n",
        "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
        "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
        "    inputs = sympy.symbols(f'x(0:{n_layers})' + f'_(0:{n_qubits})')\n",
        "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
        "    circuit = cirq.Circuit()\n",
        "    for l in range(n_layers):\n",
        "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
        "        circuit += entangling_layer(qubits)\n",
        "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
        "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i, q in enumerate(qubits))\n",
        "    return circuit, list(params.flat), list(inputs.flat)\n",
        "\n",
        "class ReUploadingPQC(tf.keras.layers.Layer):\n",
        "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
        "        super(ReUploadingPQC, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_qubits = len(qubits)\n",
        "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
        "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
        "        self.theta = tf.Variable(initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"), trainable=True, name=\"thetas\")\n",
        "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
        "        self.lmbd = tf.Variable(initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\")\n",
        "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
        "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
        "        self.activation = activation\n",
        "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
        "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
        "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
        "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
        "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
        "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
        "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
        "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
        "        return self.computation_layer([tiled_up_circuits, joined_vars])\n",
        "\n",
        "class Alternating(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        super(Alternating, self).__init__()\n",
        "        self.w = tf.Variable(initial_value=tf.constant([[(-1.)**i for i in range(output_dim)]]), dtype=\"float32\", trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w)\n",
        "\n",
        "def create_pqc_model(qubits, n_layers, n_actions, beta, observables):\n",
        "    input_tensor = tf.keras.Input(shape=(2,), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables)([input_tensor])\n",
        "    process = tf.keras.Sequential([\n",
        "        Alternating(n_actions),\n",
        "        tf.keras.layers.Lambda(lambda x: x * beta),\n",
        "        tf.keras.layers.Softmax()\n",
        "    ], name=\"observables-policy\")\n",
        "    policy = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=policy)\n",
        "    return model\n",
        "\n",
        "def compute_returns(rewards, gamma):\n",
        "    n = len(rewards)\n",
        "    returns = np.zeros(n)\n",
        "    future_return = 0.0\n",
        "    for t in reversed(range(n)):\n",
        "        future_return = rewards[t] + gamma * future_return\n",
        "        returns[t] = future_return\n",
        "    return returns\n",
        "\n",
        "def gather_episodes(state_bounds, n_actions, model, n_episodes, env_name):\n",
        "    trajectories = [defaultdict(list) for _ in range(n_episodes)]\n",
        "    envs = [gym.make(env_name) for _ in range(n_episodes)]\n",
        "    done = [False for _ in range(n_episodes)]\n",
        "    states = [e.reset() for e in envs]\n",
        "    while not all(done):\n",
        "        unfinished_ids = [i for i in range(n_episodes) if not done[i]]\n",
        "        normalized_states = [s / state_bounds for i, s in enumerate(states) if not done[i]]\n",
        "        for i, state in zip(unfinished_ids, normalized_states):\n",
        "            trajectories[i]['states'].append(state)\n",
        "        states_tensor = tf.convert_to_tensor(normalized_states, dtype=tf.float32)\n",
        "        action_probs = model(states_tensor)\n",
        "        action_probs_np = action_probs.numpy()\n",
        "        for i, (state, policy) in zip(unfinished_ids, zip(states_tensor, action_probs_np)):\n",
        "            action = np.random.choice(n_actions, p=policy)\n",
        "            next_state, reward, done[i], _ = envs[i].step(action)\n",
        "            trajectories[i]['actions'].append(action)\n",
        "            trajectories[i]['rewards'].append(reward)\n",
        "            states[i] = next_state\n",
        "    return trajectories\n",
        "\n",
        "state_bounds = np.array([1.2, 0.07])  # Normalize state bounds for MountainCar\n",
        "gamma = 0.99\n",
        "batch_size = 10\n",
        "n_episodes = 1000\n",
        "env_name = \"MountainCar-v0\"\n",
        "\n",
        "n_qubits, n_layers, n_actions = 2, 5, 3\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "observables = [reduce((lambda x, y: x * y), [cirq.Z(q) for q in qubits])]\n",
        "model = create_pqc_model(qubits, n_layers, n_actions, 1.0, observables)\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)\n",
        "\n",
        "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True)\n",
        "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "\n",
        "w_in, w_var, w_out = 1, 0, 2\n",
        "\n",
        "@tf.function\n",
        "def reinforce_update(states, actions, returns, model):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    returns = tf.convert_to_tensor(returns)\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        logits = model(states)\n",
        "        p_actions = tf.gather_nd(logits, actions)\n",
        "        log_probs = tf.math.log(p_actions)\n",
        "        loss = tf.math.reduce_sum(-log_probs * returns) / batch_size\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
        "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])\n",
        "\n",
        "episode_reward_history = []\n",
        "for batch in range(n_episodes // batch_size):\n",
        "    episodes = gather_episodes(state_bounds, n_actions, model, batch_size, env_name)\n",
        "    states = np.concatenate([ep['states'] for ep in episodes])\n",
        "    actions = np.concatenate([ep['actions'] for ep in episodes])\n",
        "    rewards = [ep['rewards'] for ep in episodes]\n",
        "    returns = np.concatenate([compute_returns(ep_rwds, gamma) for ep_rwds in rewards])\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "    id_action_pairs = np.array([[i, a] for i, a in enumerate(actions)])\n",
        "    reinforce_update(states, id_action_pairs, returns, model)\n",
        "    for ep_rwds in rewards:\n",
        "        episode_reward_history.append(np.sum(ep_rwds))\n",
        "    avg_rewards = np.mean(episode_reward_history[-10:])\n",
        "    print('Finished episode', (batch + 1) * batch_size, 'Average rewards: ', avg_rewards)\n",
        "    if avg_rewards >= -100.0:\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(episode_reward_history, label='Mean Rewards per Episode', color='maroon')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Rewards per Episode')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('Mountain_Car-PG.png', dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u3QBKbvwpUP"
      },
      "source": [
        "## 2. Deep Q-learning with PQC Q-function approximators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EesnBLLOolgS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cirq\n",
        "import gym\n",
        "from collections import deque\n",
        "from tensorflow_quantum.python.layers import PQC\n",
        "\n",
        "class Rescaling(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Rescaling, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.ones(shape=(1,input_dim)), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.multiply((inputs+1)/2, tf.repeat(self.w,repeats=tf.shape(inputs)[0],axis=0))\n",
        "\n",
        "\n",
        "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
        "    \"\"\"Generates a Keras model for a data re-uploading PQC Q-function approximator.\"\"\"\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits),), dtype=tf.dtypes.float32, name='input')\n",
        "    pqc = PQC(model_circuit=cirq.Circuit(), operators=observables, repetitions=1)\n",
        "    pqc_output = pqc(input_tensor)\n",
        "    rescaled_output = Rescaling(len(observables))(pqc_output)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=rescaled_output)\n",
        "    return model\n",
        "\n",
        "def interact_env(state, model, epsilon, n_actions, env):\n",
        "    # Preprocess state\n",
        "    state_array = np.array(state)\n",
        "    state = tf.convert_to_tensor([state_array])\n",
        "\n",
        "    # Sample action\n",
        "    coin = np.random.random()\n",
        "    if coin > epsilon:\n",
        "        q_vals = model([state])\n",
        "        action = int(tf.argmax(q_vals[0]).numpy())\n",
        "    else:\n",
        "        action = np.random.choice(n_actions)\n",
        "\n",
        "    # Apply sampled action in the environment, receive reward and next state\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    interaction = {'state': state_array, 'action': action, 'next_state': next_state.copy(),\n",
        "                   'reward': reward, 'done':np.float32(done)}\n",
        "\n",
        "    return interaction\n",
        "\n",
        "@tf.function\n",
        "def Q_learning_update(states, actions, rewards, next_states, done, model, gamma, n_actions):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    rewards = tf.convert_to_tensor(rewards)\n",
        "    next_states = tf.convert_to_tensor(next_states)\n",
        "    done = tf.convert_to_tensor(done)\n",
        "\n",
        "    # Compute their target q_values and the masks on sampled actions\n",
        "    future_rewards = model([next_states])\n",
        "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "                                                   * (1.0 - done))\n",
        "    masks = tf.one_hot(actions, n_actions)\n",
        "\n",
        "    # Train the model on the states and target Q-values\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        q_values = model([states])\n",
        "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
        "\n",
        "    # Backpropagation\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "gamma = 0.99\n",
        "n_episodes = 2000\n",
        "\n",
        "# Define replay memory\n",
        "max_memory_length = 10000 # Maximum replay length\n",
        "replay_memory = deque(maxlen=max_memory_length)\n",
        "\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
        "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
        "batch_size = 16\n",
        "steps_per_update = 10 # Train the model every x steps\n",
        "steps_per_target_update = 30 # Update the target model every x steps\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "episode_reward_history = []\n",
        "step_count = 0\n",
        "for episode in range(n_episodes):\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        # Interact with env\n",
        "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
        "\n",
        "        # Store interaction in the replay memory\n",
        "        replay_memory.append(interaction)\n",
        "\n",
        "        state = interaction['next_state']\n",
        "        episode_reward += interaction['reward']\n",
        "        step_count += 1\n",
        "\n",
        "        # Update model\n",
        "        # if step_count % steps_per_update == 0:\n",
        "        #     # Sample a batch of interactions and update Q_function\n",
        "        #     training_batch = np.random.choice(replay_memory, size=batch_size, replace=False)\n",
        "        #     Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
        "        #                       np.asarray([x['action'] for x in training_batch]),\n",
        "        #                       np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
        "        #                       np.asarray([x['next_state'] for x in training_batch]),\n",
        "        #                       np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
        "        #                       model, gamma, n_actions)\n",
        "\n",
        "        # Update model\n",
        "        if step_count % steps_per_update == 0:\n",
        "          if len(replay_memory) >= batch_size:\n",
        "          # Sample a batch of interactions and update Q_function\n",
        "            training_batch = np.random.choice(replay_memory, size=batch_size, replace=False)\n",
        "            Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
        "                              np.asarray([x['action'] for x in training_batch]),\n",
        "                              np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
        "                              np.asarray([x['next_state'] for x in training_batch]),\n",
        "                              np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
        "                              model, gamma, n_actions)\n",
        "\n",
        "\n",
        "        # Check if the episode is finished\n",
        "        if interaction['done']:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if (episode+1)%10 == 0:\n",
        "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
        "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
        "            episode+1, n_episodes, avg_rewards))\n",
        "        if avg_rewards >= -100.0:  # Define your own condition for task completion\n",
        "            break\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(episode_reward_history, label='Mean Rewards per Episode', color='maroon')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Rewards per Episode')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('Mountain_Car-PG.png', dpi=600)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (main, Dec  7 2022, 13:47:07) [GCC 12.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
