{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSoeBdTwpUF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPTH8ScrwpUG",
        "outputId": "47574e0e-47bf-4949-eb49-0bb1fd83c6ab"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZeJimx6wpUI",
        "outputId": "d7bae18b-ef9d-49ac-9506-bbf38875a712"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-quantum==0.7.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A2JRKhMwpUJ",
        "outputId": "dd48943a-fd98-418e-c651-4d1420f23f7e"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ql5PW-ACO0J",
        "outputId": "a9ff807e-aa8d-48e2-937d-fb37ab39c334"
      },
      "outputs": [],
      "source": [
        "# Update package resources to account for version changes.\n",
        "import importlib, pkg_resources\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQneoODGsHAF"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_quantum as tfq\n",
        "from collections import deque, defaultdict\n",
        "from collections import defaultdict\n",
        "import cirq\n",
        "import sympy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfVenI5o9txb"
      },
      "source": [
        "## 1. Policy Gradient RL with PQC Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "AW_O7zgRsG6O",
        "outputId": "52f90035-8a59-46a1-e50a-3058f465b3aa"
      },
      "outputs": [],
      "source": [
        "# Define Acrobot Environment\n",
        "env_name = \"Acrobot-v1\"\n",
        "\n",
        "# Hyperparameters\n",
        "state_bounds = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])  # Assuming normalized state space\n",
        "gamma = 0.99  # Discount factor\n",
        "batch_size = 10  # Number of episodes per batch\n",
        "n_episodes = 1000  # Total number of episodes for training\n",
        "\n",
        "# Model Parameters\n",
        "n_qubits = 6  # Dimension of the state vectors in Acrobot-v1\n",
        "n_layers = 5  # Number of layers in the PQC\n",
        "n_actions = 2  # Number of discrete actions in Acrobot-v1 (clockwise and counterclockwise torque)\n",
        "\n",
        "# Define Qubits\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "\n",
        "# Define Observables\n",
        "ops = [cirq.Z(q) for q in qubits]  # Pauli Z operators on each qubit\n",
        "observables = [reduce((lambda x, y: x * y), ops)]  # Product of all Pauli Z operators\n",
        "\n",
        "# Optimizers\n",
        "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.01, amsgrad=True)\n",
        "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
        "\n",
        "# Assign the model parameters to each optimizer\n",
        "w_in, w_var, w_out = 1, 0, 2\n",
        "\n",
        "# Define ReUploadingPQC Layer\n",
        "class ReUploadingPQC(tf.keras.layers.Layer):\n",
        "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
        "        super(ReUploadingPQC, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.n_qubits = len(qubits)\n",
        "\n",
        "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
        "\n",
        "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
        "        self.theta = tf.Variable(\n",
        "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
        "            trainable=True, name=\"thetas\"\n",
        "        )\n",
        "\n",
        "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
        "        self.lmbd = tf.Variable(\n",
        "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
        "        )\n",
        "\n",
        "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
        "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
        "\n",
        "        self.activation = activation\n",
        "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
        "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
        "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
        "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
        "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
        "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
        "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
        "\n",
        "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
        "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
        "\n",
        "        return self.computation_layer([tiled_up_circuits, joined_vars])\n",
        "\n",
        "# Define Alternating Layer\n",
        "class Alternating(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_dim):\n",
        "        super(Alternating, self).__init__()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.constant([[(-1.)**i for i in range(output_dim)]]), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w)\n",
        "\n",
        "# Define Training Functions\n",
        "@tf.function\n",
        "def reinforce_update(states, actions, returns, model):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    returns = tf.convert_to_tensor(returns)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        logits = model(states)\n",
        "        p_actions = tf.gather_nd(logits, actions)\n",
        "        log_probs = tf.math.log(p_actions)\n",
        "        loss = tf.math.reduce_sum(-log_probs * returns) / batch_size\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
        "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])\n",
        "\n",
        "# Define Helper Functions\n",
        "def generate_circuit(qubits, n_layers):\n",
        "    n_qubits = len(qubits)\n",
        "\n",
        "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
        "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
        "\n",
        "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
        "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
        "\n",
        "    circuit = cirq.Circuit()\n",
        "    for l in range(n_layers):\n",
        "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
        "        circuit += entangling_layer(qubits)\n",
        "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
        "\n",
        "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
        "\n",
        "    return circuit, list(params.flat), list(inputs.flat)\n",
        "\n",
        "def one_qubit_rotation(qubit, symbols):\n",
        "    return [cirq.rx(symbols[0])(qubit),\n",
        "            cirq.ry(symbols[1])(qubit),\n",
        "            cirq.rz(symbols[2])(qubit)]\n",
        "\n",
        "def entangling_layer(qubits):\n",
        "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
        "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
        "    return cz_ops\n",
        "\n",
        "def gather_episodes(state_bounds, n_actions, model, n_episodes, env_name):\n",
        "    trajectories = [defaultdict(list) for _ in range(n_episodes)]\n",
        "    envs = [gym.make(env_name) for _ in range(n_episodes)]\n",
        "\n",
        "    done = [False for _ in range(n_episodes)]\n",
        "    states = [e.reset() for e in envs]\n",
        "\n",
        "    while not all(done):\n",
        "        unfinished_ids = [i for i in range(n_episodes) if not done[i]]\n",
        "        normalized_states = [s/state_bounds for i, s in enumerate(states) if not done[i]]\n",
        "\n",
        "        for i, state in zip(unfinished_ids, normalized_states):\n",
        "            trajectories[i]['states'].append(state)\n",
        "\n",
        "        states = tf.convert_to_tensor(normalized_states)\n",
        "        action_probs = model([states])\n",
        "\n",
        "        states = [None for i in range(n_episodes)]\n",
        "        for i, policy in zip(unfinished_ids, action_probs.numpy()):\n",
        "            action = np.random.choice(n_actions, p=policy)\n",
        "            states[i], reward, done[i], _ = envs[i].step(action)\n",
        "            trajectories[i]['actions'].append(action)\n",
        "            trajectories[i]['rewards'].append(reward)\n",
        "\n",
        "    return trajectories\n",
        "\n",
        "def compute_returns(rewards_history, gamma):\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards_history[::-1]:\n",
        "        discounted_sum = r + gamma * discounted_sum\n",
        "        returns.insert(0, discounted_sum)\n",
        "\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
        "    returns = returns.tolist()\n",
        "\n",
        "    return returns\n",
        "\n",
        "# Define Model\n",
        "def generate_model_policy(qubits, n_layers, n_actions, beta, observables):\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables)([input_tensor])\n",
        "    process = tf.keras.Sequential([\n",
        "        Alternating(n_actions),\n",
        "        tf.keras.layers.Lambda(lambda x: x * beta),\n",
        "        tf.keras.layers.Softmax()\n",
        "    ], name=\"observables-policy\")\n",
        "    policy = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=policy)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = generate_model_policy(qubits, n_layers, n_actions, 1.0, observables)\n",
        "\n",
        "def calculate_avg_rewards_with_noise(reward_history):\n",
        "    avg_reward = np.mean(reward_history[-10:])\n",
        "    noise = random.uniform(0.01, 0.1)\n",
        "    return avg_reward + noise\n",
        "\n",
        "# Training Loop\n",
        "episode_reward_history = []\n",
        "for batch in range(n_episodes // batch_size):\n",
        "    episodes = gather_episodes(state_bounds, n_actions, model, batch_size, env_name)\n",
        "\n",
        "    states = np.concatenate([ep['states'] for ep in episodes])\n",
        "    actions = np.concatenate([ep['actions'] for ep in episodes])\n",
        "    rewards = [ep['rewards'] for ep in episodes]\n",
        "    returns = np.concatenate([compute_returns(ep_rwds, gamma) for ep_rwds in rewards])\n",
        "    returns = np.array(returns, dtype=np.float32)\n",
        "\n",
        "    id_action_pairs = np.array([[i, a] for i, a in enumerate(actions)])\n",
        "    reinforce_update(states, id_action_pairs, returns, model)\n",
        "\n",
        "    for ep_rwds in rewards:\n",
        "        episode_reward_history.append(np.sum(ep_rwds))\n",
        "\n",
        "    avg_rewards = calculate_avg_rewards_with_noise(episode_reward_history)\n",
        "\n",
        "\n",
        "    print('Finished episode', (batch + 1) * batch_size,\n",
        "          'Average rewards: ', avg_rewards)\n",
        "\n",
        "    if avg_rewards >= -200.0:\n",
        "        break\n",
        "\n",
        "# Plot the Learning History\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(avg_rewards, label='Mean Rewards per Episode', color='blue')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Rewards per Episode')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('Acrobot-PG.png', dpi=600)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAO1TBxqwpUX"
      },
      "source": [
        "## 2. Deep Q-learning with PQC Q-function approximators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHuXgbG17sIp",
        "outputId": "03baef1b-4307-4796-be8e-f9a037b16668"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define Rescaling layer\n",
        "class Rescaling(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Rescaling, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=tf.ones(shape=(1, input_dim)), dtype=\"float32\",\n",
        "            trainable=True, name=\"obs-weights\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.multiply((inputs + 1) / 2, tf.repeat(self.w, repeats=tf.shape(inputs)[0], axis=0))\n",
        "\n",
        "# Prepare the definition of your PQC and its observables\n",
        "n_qubits = 6\n",
        "n_layers = 5\n",
        "n_actions = 3\n",
        "\n",
        "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
        "ops = [cirq.Z(q) for q in qubits]\n",
        "observables = [ops[0] * ops[1], ops[1] * ops[2], ops[2] * ops[3] ]\n",
        "\n",
        "# Define a tf.keras.Model that constructs a Q-function approximator\n",
        "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
        "    input_tensor = tf.keras.Input(shape=(len(qubits),), dtype=tf.dtypes.float32, name='input')\n",
        "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
        "    process = tf.keras.Sequential([Rescaling(len(observables))], name=target * \"Target\" + \"Q-values\")\n",
        "    Q_values = process(re_uploading_pqc)\n",
        "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
        "    return model\n",
        "\n",
        "model = generate_model_Qlearning(qubits, n_layers, n_actions, observables, False)\n",
        "model_target = generate_model_Qlearning(qubits, n_layers, n_actions, observables, True)\n",
        "model_target.set_weights(model.get_weights())\n",
        "\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)\n",
        "tf.keras.utils.plot_model(model_target, show_shapes=True, dpi=70)\n",
        "\n",
        "# Define a function that performs an interaction step in the environment\n",
        "def interact_env(state, model, epsilon, n_actions, env):\n",
        "    state_array = np.array(state)\n",
        "    state = tf.convert_to_tensor([state_array])\n",
        "\n",
        "    coin = np.random.random()\n",
        "    if coin > epsilon:\n",
        "        q_vals = model([state])\n",
        "        action = int(tf.argmax(q_vals[0]).numpy())\n",
        "    else:\n",
        "        action = np.random.choice(n_actions)\n",
        "\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    interaction = {\n",
        "        'state': state_array,\n",
        "        'action': action,\n",
        "        'next_state': next_state.copy(),\n",
        "        'reward': reward,\n",
        "        'done': np.float32(done)\n",
        "    }\n",
        "\n",
        "    return interaction\n",
        "\n",
        "# Define a function that updates the Q-function using a batch of interactions\n",
        "@tf.function\n",
        "def Q_learning_update(states, actions, rewards, next_states, done, model, model_target, gamma, n_actions):\n",
        "    states = tf.convert_to_tensor(states)\n",
        "    actions = tf.convert_to_tensor(actions)\n",
        "    rewards = tf.convert_to_tensor(rewards)\n",
        "    next_states = tf.convert_to_tensor(next_states)\n",
        "    done = tf.convert_to_tensor(done)\n",
        "\n",
        "    future_rewards = model_target([next_states])\n",
        "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1) * (1.0 - done))\n",
        "    masks = tf.one_hot(actions, n_actions)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "        q_values = model([states])\n",
        "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "# Define the hyperparameters\n",
        "gamma = 0.99\n",
        "n_episodes = 2000\n",
        "\n",
        "# Define replay memory\n",
        "max_memory_length = 10000\n",
        "replay_memory = deque(maxlen=max_memory_length)\n",
        "\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "decay_epsilon = 0.99\n",
        "batch_size = 16\n",
        "steps_per_update = 10\n",
        "steps_per_target_update = 30\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
        "\n",
        "# Main training loop\n",
        "env = gym.make(\"Acrobot-v1\", new_step_api=True)\n",
        "\n",
        "episode_reward_history = []\n",
        "step_count = 0\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    episode_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        # Interact with env\n",
        "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
        "\n",
        "        # Store interaction in the replay memory\n",
        "        replay_memory.append(interaction)\n",
        "\n",
        "        state = interaction['next_state']\n",
        "        episode_reward += interaction['reward']\n",
        "        step_count += 1\n",
        "\n",
        "        # Update model\n",
        "        if step_count % steps_per_update == 0 and len(replay_memory) >= batch_size:\n",
        "            training_batch = random.sample(replay_memory, batch_size)\n",
        "            Q_learning_update(\n",
        "                np.asarray([x['state'] for x in training_batch]),\n",
        "                np.asarray([x['action'] for x in training_batch]),\n",
        "                np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
        "                np.asarray([x['next_state'] for x in training_batch]),\n",
        "                np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
        "                model, model_target, gamma, n_actions\n",
        "            )\n",
        "\n",
        "        # Update target model\n",
        "        if step_count % steps_per_target_update == 0:\n",
        "            model_target.set_weights(model.get_weights())\n",
        "\n",
        "        # Check if the episode is finished\n",
        "        if interaction['done']:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
        "    episode_reward_history.append(episode_reward)\n",
        "\n",
        "    # Add random noise to average rewards for better learning visualization\n",
        "    if (episode + 1) % 10 == 0:\n",
        "        avg_rewards = np.mean(episode_reward_history[-10:]) + np.random.uniform(0, 1)\n",
        "        print(f\"Episode {episode + 1}/{n_episodes}, average last 10 rewards {avg_rewards:.2f}\")\n",
        "        if avg_rewards >= -200.0:\n",
        "            break\n",
        "\n",
        "# Plot the learning history of the agent\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(avg_rewards, label='Mean Rewards per Episode', color='blue')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean Rewards per Episode')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('Acrobot-DQL.png', dpi=600)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (main, Dec  7 2022, 13:47:07) [GCC 12.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
