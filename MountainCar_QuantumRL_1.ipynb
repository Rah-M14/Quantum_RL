{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c1bca5-2df7-4440-b1bf-717a8341d6d1",
   "metadata": {},
   "source": [
    "### Quantum Reinforcement Learning in Mountain-Car environment of the gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be7d2e-7828-4f22-8d78-246cad4ad591",
   "metadata": {},
   "source": [
    "### 1. Basic Heuristic based RL Implementation of Mountain-Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dd85b-e551-41b7-a354-87f9145efe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cirq\n",
    "import sympy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca41b0-3040-469a-ac12-74e001094705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_qubit_rotation(qubit, symbols):\n",
    "    \"\"\"Rotates a qubit on the Bloch sphere.\"\"\"\n",
    "    return [cirq.rx(symbols[0])(qubit), cirq.ry(symbols[1])(qubit), cirq.rz(symbols[2])(qubit)]\n",
    "\n",
    "def entangling_layer(qubits):\n",
    "    \"\"\"Creates a layer of entangling gates (CZ gates) among all qubits.\"\"\"\n",
    "    return [cirq.CZ(qubits[i], qubits[(i + 1) % len(qubits)]) for i in range(len(qubits))]\n",
    "\n",
    "def generate_circuit(qubits, n_layers):\n",
    "    \"\"\"Generates a quantum circuit for the given qubits and layers.\"\"\"\n",
    "       \n",
    "    # Parameters for rotation gates\n",
    "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*len(qubits)})')\n",
    "    params = np.asarray(params).reshape((n_layers + 1, len(qubits), 3))\n",
    "    \n",
    "    \n",
    "    circuit = cirq.Circuit()\n",
    "    for l in range(n_layers):\n",
    "        circuit += [one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits)]\n",
    "        circuit += entangling_layer(qubits)\n",
    "    \n",
    "    circuit += [one_qubit_rotation(q, params[n_layers, i]) for i, q in enumerate(qubits)]\n",
    "    \n",
    "    return circuit, list(params.flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0da01-302d-487b-b830-4af88bee00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(state):\n",
    "    \"\"\"Maps the continuous state variables of the environment to angles.\"\"\"\n",
    "    position, velocity = state\n",
    "    position = np.interp(position, [env.observation_space.low[0], env.observation_space.high[0]], [-np.pi, np.pi])\n",
    "    velocity = np.interp(velocity, [env.observation_space.low[1], env.observation_space.high[1]], [-np.pi, np.pi])\n",
    "    return np.array([position, velocity])\n",
    "\n",
    "def choose_action(state, params, qubits, circuit):\n",
    "    \"\"\"Decides an action by measuring the output of the quantum circuit.\"\"\"\n",
    "    resolver = cirq.ParamResolver({str(sympy.Symbol(f'theta({i})')): state[i % len(state)] for i in range(len(params))})\n",
    "    final_state = cirq.Simulator().simulate(circuit, resolver).final_state_vector\n",
    "    probabilities = np.abs(final_state)**2\n",
    "    action = np.random.choice(3, p=probabilities[:3]/np.sum(probabilities[:3]))  # Assuming 3 actions\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931fedd7-8e43-4b41-8b1d-75de58c8da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "n_qubits = 2\n",
    "n_layers = 1\n",
    "episodes = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize qubits and generate circuit\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "circuit, symbols = generate_circuit(qubits, n_layers)\n",
    "params = np.random.uniform(low=-np.pi, high=np.pi, size=len(symbols))\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = process_state(env.reset())\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(env.spec.max_episode_steps):\n",
    "        action = choose_action(state, params, qubits, circuit)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = process_state(next_state)\n",
    "\n",
    "        # Heuristic parameter update\n",
    "        if done and reward == 0:  \n",
    "            reward = -100  \n",
    "        params += learning_rate * reward * np.random.normal(size=params.shape)  \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    print(f'Episode: {episode + 1}, Total Reward: {total_reward}, Parameters: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0833f83-93a1-4bdc-ba96-03897d602ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.plot(range(1, episodes + 1), rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward vs. Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d13ef-a2dc-42cb-b541-39802f73e2cc",
   "metadata": {},
   "source": [
    "### 2. Policy Gradient RL with PQC policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a20da-b401-45f2-851a-acea0543c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize qubits and generate the quantum circuit for the policy network\n",
    "n_qubits = 2 \n",
    "n_layers = 1  \n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "circuit, symbols = generate_circuit(qubits, n_layers)\n",
    "\n",
    "# Convert the Cirq circuit to a TensorFlow Quantum circuit\n",
    "q_circuit = tfq.convert_to_tensor([circuit])\n",
    "\n",
    "\n",
    "policy_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(), dtype=tf.dtypes.string),\n",
    "    tfq.layers.PQC(circuit, symbols),\n",
    "    tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "policy_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "# Training loop\n",
    "episodes = 100\n",
    "rewards = []  \n",
    "\n",
    "for episode in range(episodes):\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # State encoding\n",
    "            state_tensor = tfq.convert_to_tensor([cirq.Circuit(cirq.rx(s)(q) for s, q in zip(state, qubits))])\n",
    "            # Action selection\n",
    "            action_probs = policy_model(state_tensor, training=True)\n",
    "            action = np.random.choice(env.action_space.n, p=action_probs.numpy().flatten())\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Policy gradient update\n",
    "            loss_value = -tf.math.log(action_probs[0, action])\n",
    "            grads = tape.gradient(loss_value, policy_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        rewards.append(total_reward) \n",
    "\n",
    "    print(f'Episode: {episode + 1}, Total Reward: {total_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec914fe-1e87-450c-b210-56797e128562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.plot(range(1, episodes + 1), rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward vs. Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
